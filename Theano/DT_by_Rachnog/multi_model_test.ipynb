{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../csv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Permute\n",
    "from keras.layers import Merge, Input, concatenate, average, add\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D, RepeatVector, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "\n",
    "#Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_text_csv(filename = 'multimodel/Combined_News_DJIA.csv', date_split = date(2014,12,31)):\n",
    "    '''\n",
    "    Load news from csv, group them and split in train/test set due to @date_split\n",
    "    '''\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)\n",
    "    \n",
    "    train = df.loc[(pd.to_datetime(df[\"Date\"]) <= date_split),['Label','Combined']]\n",
    "    test = df.loc[(pd.to_datetime(df[\"Date\"]) > date_split),['Label','Combined']]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 378\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "      Label                                           Combined\n",
      "1611      1  [ 'Most cases of cancer are the result of shee...\n",
      "1612      0  [ 'Moscow-&gt;Beijing high speed train will re...\n",
      "1613      0  ['US oil falls below $50 a barrel'\\n \"Toyota g...\n",
      "1614      1  [\"'Shots fired' at French magazine HQ\"\\n '90% ...\n",
      "1615      1  [ 'New Charlie Hebdo issue to come out next we...\n"
     ]
    }
   ],
   "source": [
    "'''Using news as a input!!'''\n",
    "train, test = load_text_csv()\n",
    "print(len(train), len(test))\n",
    "print(type(test))\n",
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2change(data):\n",
    "    change = pd.DataFrame(data).pct_change()\n",
    "    change = change.replace([np.inf, -np.inf], np.nan)\n",
    "    change = change.fillna(0.).values.tolist()\n",
    "    change = [c[0] for c in change]\n",
    "    return change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ts_csv(filename = 'multimodel/DJIA_table.csv', date_split = date(2014,12,31)):\n",
    "    '''\n",
    "    Load time series from csv, taking adjustment close prices;\n",
    "    transforming them into percentage of price change;\n",
    "    split in train/test set due to @date_split\n",
    "    '''\n",
    "    \n",
    "    data_original = pd.read_csv(filename)[::-1]\n",
    "    \n",
    "    train2 = data_original.loc[(pd.to_datetime(data_original[\"Date\"]) <= date_split)]\n",
    "    test2 = data_original.loc[(pd.to_datetime(data_original[\"Date\"]) > date_split)]\n",
    "    \n",
    "    open_train = train2.loc[:, 'Open']\n",
    "    open_test = test2.loc[:, 'Open']\n",
    "    \n",
    "    high_train = train2.loc[:, 'High']\n",
    "    high_test = test2.loc[:, 'High']\n",
    "    \n",
    "    low_train = train2.loc[:, 'Low']\n",
    "    low_test = test2.loc[:, 'Low']\n",
    "    \n",
    "    close_train = train2.loc[:, 'Close']\n",
    "    close_test = test2.loc[:, 'Close']\n",
    "    \n",
    "    volume_train = train2.loc[:, 'Volume']\n",
    "    volume_test = test2.loc[:, 'Volume']\n",
    "    \n",
    "    open_train = data2change(open_train)\n",
    "    open_test = data2change(open_test)\n",
    "    \n",
    "    high_train = data2change(high_train)\n",
    "    high_test = data2change(high_test)\n",
    "    \n",
    "    low_train = data2change(low_train)\n",
    "    low_test = data2change(low_test)\n",
    "    \n",
    "    close_test = data2change(close_test)\n",
    "    close_train = data2change(close_train)\n",
    "    \n",
    "    volume_train = data2change(volume_train)\n",
    "    volume_test = data2change(volume_test)\n",
    "    \n",
    "    train = np.column_stack((open_train, high_train, low_train, close_train, volume_train))\n",
    "    test = np.column_stack((open_test, high_test, low_test, close_test, volume_test))\n",
    "    \n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 5)\n",
      "(378, 5)\n",
      "1611 378\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [ -9.92831745e-05  -7.26827764e-03  -1.44022763e-02  -1.85801617e-02\n",
      "    5.23010358e-01]\n",
      " [ -1.77944975e-02  -1.34810586e-02  -1.22202682e-02  -7.42842893e-03\n",
      "   -1.23019972e-01]\n",
      " [ -7.39254243e-03   9.11737142e-04   6.51186144e-03   1.22543927e-02\n",
      "   -1.06410131e-01]\n",
      " [  1.25003836e-02   1.81256767e-02   1.25003836e-02   1.83883108e-02\n",
      "    2.62111392e-01]]\n"
     ]
    }
   ],
   "source": [
    "'''[open price, high price, low price, close price, volume]'''\n",
    "data_chng_train, data_chng_test = load_ts_csv()\n",
    "print(len(data_chng_train), len(data_chng_test))\n",
    "print(data_chng_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_text_into_vectors(train_text, test_text, embedding_size = 100, model_path = 'word2vec10.model'):\n",
    "    '''\n",
    "        Transforms sentences into sequences of word2vec vectors\n",
    "        Returns train, test set and trained word2vec model\n",
    "    '''\n",
    "    data_for_w2v = []\n",
    "    for text in train_text + test_text:\n",
    "        words = text.split(' ')\n",
    "        data_for_w2v.append(words)\n",
    "\n",
    "    model = Word2Vec(data_for_w2v, size=embedding_size, window=5, min_count=1, workers=4)\n",
    "    model.save(model_path)\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "    train_text_vectors = [[model[x] for x in sentence.split(' ')] for sentence in train_text]\n",
    "    test_text_vectors = [[model[x] for x in sentence.split(' ')] for sentence in test_text]\n",
    "\n",
    "    train_text_vectors = [np.mean(x, axis=0) for x in train_text_vectors]\n",
    "    test_text_vectors = [np.mean(x, axis=0) for x in test_text_vectors]\n",
    "\n",
    "    return train_text_vectors, test_text_vectors, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 378\n",
      "['case cancer result sheer bad luck rather unhealthi lifestyl diet even inherit gene new research suggest random mutat occur dna cell divid respons two third adult cancer across wide rang tissu iran dismiss unit state effort fight islam state ploy advanc u polici region realiti unit state act elimin daesh even interest weaken daesh interest manag poll one 8 german would join anti muslim march uk royal famili princ andrew name us lawsuit underag sex alleg 40 asylum seeker refus leav bu arriv destin rural northern sweden demand taken back malm big citi pakistani boat blow self india navi chase four peopl board vessel near pakistani port citi karachi believ kill dramat episod arabian sea new year eve accord india defenc ministri sweden hit third mosqu arson attack week 940 car set alight french new year salari top ceo rose twice fast averag canadian sinc recess studi norway violat equal pay law judg say judg find consul employe unjustli paid 30 000 less male counterpart imam want radic recruit muslim youth canada identifi dealt saudi arabia behead 83 peopl 2014 year live hell slave remot south korean island slaveri thrive chain rural island south korea rug southwest coast nurtur long histori exploit demand tri squeez live sea world 400 richest get richer ad 92bn 2014 rental car stereo infring copyright music right group say ukrainian minist threaten tv channel closur air russian entertain palestinian presid mahmoud abba enter seriou confront yet israel sign onto intern crimin court decis wednesday give court jurisdict crime commit palestinian land isra secur center publish name 50 kill terrorist conceal hama year 2014 deadliest year yet syria four year conflict 76 000 kill secret underground complex built nazi may use develop wmd includ nuclear bomb uncov austria restrict web freedom major global issu 2015 austrian journalist erich mchel deliv present hamburg annual meet chao comput club monday decemb 29 detail variou locat us nsa activ collect process electron intellig vienna thousand ukrain nationalist march kiev china new year resolut harvest execut prison organ author pull plug russia last polit independ tv station']\n"
     ]
    }
   ],
   "source": [
    "train_text = cPickle.load(open('multimodel/train_text.p', 'rb'))[:]\n",
    "test_text = cPickle.load(open('multimodel/test_text.p', 'rb'))[:]\n",
    "print(len(train_text), len(test_text))\n",
    "print(test_text[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 378\n",
      "<class 'gensim.models.word2vec.Word2Vec'>\n",
      "[array([  7.04339743e-02,   3.90487701e-01,  -5.20477891e-02,\n",
      "        -3.22473109e-01,   6.54451773e-02,  -3.48789215e-01,\n",
      "         3.16370666e-01,  -3.95053744e-01,  -7.10589170e-01,\n",
      "         2.81793982e-01,  -8.34351331e-02,  -4.39769208e-01,\n",
      "        -4.28961635e-01,   4.16660935e-01,   2.72161812e-01,\n",
      "        -3.54525268e-01,  -2.05096290e-01,  -4.45461392e-01,\n",
      "         3.99229497e-01,  -1.68372840e-01,   1.27289474e-01,\n",
      "        -1.84600785e-01,  -4.23597127e-01,   2.07033977e-01,\n",
      "        -1.77551717e-01,   5.36276877e-01,   3.06709737e-01,\n",
      "        -6.80167526e-02,   9.50721186e-03,   1.25350937e-01,\n",
      "        -1.38251364e-01,   2.89648592e-01,  -5.32856584e-02,\n",
      "         3.15650910e-01,   1.41548701e-02,  -3.28086950e-02,\n",
      "        -2.14281231e-02,   4.97224033e-02,  -2.95984864e-01,\n",
      "         3.00398558e-01,  -2.59565383e-01,   2.43289217e-01,\n",
      "        -2.00663671e-01,  -2.01147169e-01,   1.34392247e-01,\n",
      "         2.11744174e-01,  -5.25576115e-01,   3.96359675e-02,\n",
      "         2.00337067e-01,  -5.31386614e-01,  -2.45555714e-01,\n",
      "         5.15287638e-01,   3.04564536e-01,   2.31091455e-01,\n",
      "         1.38959065e-01,   4.50967774e-02,   2.98872560e-01,\n",
      "        -6.28586471e-01,   6.15751408e-02,   1.18602194e-01,\n",
      "         4.38356459e-01,   1.52266935e-01,  -3.74480754e-01,\n",
      "        -2.42150128e-01,   3.49571824e-01,   4.88171935e-01,\n",
      "        -3.61538321e-01,  -7.41063952e-02,   1.18473418e-01,\n",
      "         1.81439355e-01,  -2.44048089e-01,   7.29309499e-01,\n",
      "        -4.04043108e-01,  -3.00168902e-01,   2.34607771e-01,\n",
      "         5.59390306e-01,  -3.16317886e-01,  -6.27222359e-02,\n",
      "         5.30708969e-01,  -1.11729309e-01,  -1.03173411e+00,\n",
      "         9.57334042e-02,  -1.89394772e-01,  -2.42730901e-01,\n",
      "         3.68000090e-01,  -4.19683814e-01,   2.98569202e-01,\n",
      "        -3.11590880e-02,  -1.77814439e-01,   4.56437618e-01,\n",
      "         1.28353745e-01,  -4.30228114e-01,  -1.67397112e-01,\n",
      "        -2.51046747e-01,   2.88812935e-01,   5.55730797e-03,\n",
      "        -1.49165154e-01,  -1.02720666e-03,   5.42043522e-02,\n",
      "         2.16146767e-01], dtype=float32), array([ 0.05099397,  0.38867757, -0.04622924, -0.33295479,  0.09337867,\n",
      "       -0.34661794,  0.34502989, -0.43726781, -0.71414572,  0.30528852,\n",
      "       -0.0880169 , -0.45267209, -0.44711575,  0.45816502,  0.33161965,\n",
      "       -0.41097006, -0.2176231 , -0.38969994,  0.45583177, -0.1871116 ,\n",
      "        0.1823006 , -0.21717596, -0.37485632,  0.15487042, -0.16973701,\n",
      "        0.54287076,  0.31017998, -0.0606074 ,  0.00854262,  0.16231343,\n",
      "       -0.123625  ,  0.24940741, -0.0396136 ,  0.3184714 , -0.03454496,\n",
      "       -0.02607216,  0.00517559,  0.06530364, -0.28944203,  0.28525159,\n",
      "       -0.33627653,  0.29465261, -0.23257472, -0.21579085,  0.163195  ,\n",
      "        0.18667057, -0.53787106,  0.02676294,  0.15824078, -0.52585858,\n",
      "       -0.31236404,  0.61482644,  0.31539646,  0.31965014,  0.14906557,\n",
      "        0.10565156,  0.29353482, -0.61793941,  0.02057456,  0.12562123,\n",
      "        0.46480992,  0.16626583, -0.35096815, -0.21226907,  0.3544547 ,\n",
      "        0.51332164, -0.3500762 , -0.0814795 ,  0.095669  ,  0.16529965,\n",
      "       -0.22048043,  0.75650251, -0.36891624, -0.29684633,  0.21677668,\n",
      "        0.56517541, -0.33737099, -0.12048826,  0.53394306, -0.14731601,\n",
      "       -1.03411877,  0.10401232, -0.15493566, -0.2512877 ,  0.3986696 ,\n",
      "       -0.50306022,  0.3006514 , -0.05538487, -0.13678919,  0.41768309,\n",
      "        0.0667074 , -0.44527021, -0.125965  , -0.25092804,  0.30290583,\n",
      "       -0.04980848, -0.19403408, -0.00538185,  0.02100394,  0.22207144], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "train_text_vectors, test_text_vectors, model = transform_text_into_vectors(train_text, test_text, 100)\n",
    "print(len(train_text_vectors), len(test_text_vectors))\n",
    "print(type(model))\n",
    "print(test_text_vectors[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_into_XY(data_chng_train, train_text_vectors, step, window, forecast):\n",
    "    '''\n",
    "        Splits textual and time series data into train or test dataset for hybrid model;\n",
    "        objective y_i is percentage change of price movement for next day\n",
    "    '''\n",
    "    X_train, X_train_text, Y_train, Y_train2 = [], [], [], []\n",
    "    for i in range(0, len(data_chng_train), step): \n",
    "        try:\n",
    "            x_i = data_chng_train[i:i+window]\n",
    "            y_i = np.std(data_chng_train[i:i+window+forecast][3])\n",
    "\n",
    "            text_average = train_text_vectors[i:i+window]\n",
    "            last_close = x_i[-1]\n",
    "            y_i2 = None\n",
    "            if data_chng_train[i+window+forecast][3] > 0.:\n",
    "                y_i2 = 1.\n",
    "            else:\n",
    "                y_i2 = 0.\n",
    "\n",
    "        except Exception as e:\n",
    "            print('KEK', e)\n",
    "            break\n",
    "\n",
    "        X_train.append(x_i)\n",
    "        X_train_text.append(text_average)\n",
    "        Y_train.append(y_i)\n",
    "        Y_train2.append(y_i2)\n",
    "\n",
    "    X_train, X_train_text, Y_train, Y_train2 = np.array(X_train), np.array(X_train_text), np.array(Y_train), np.array(Y_train2)\n",
    "    return X_train, X_train_text, Y_train, Y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEK index 1611 is out of bounds for axis 0 with size 1611\n",
      "KEK index 378 is out of bounds for axis 0 with size 378\n"
     ]
    }
   ],
   "source": [
    "X_train, X_train_text, Y_train, Y_train2 = split_into_XY(data_chng_train, train_text_vectors, 1, 30, 1)\n",
    "X_test, X_test_text, Y_test, Y_test2 = split_into_XY(data_chng_test, test_text_vectors, 1, 30, 1)\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 5))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_input = Input(shape=(30, 5), name='ts_input')\n",
    "text_input = Input(shape=(30, 100), name='text_input')\n",
    "\n",
    "lstm1 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(main_input)\n",
    "lstm1 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(lstm1)\n",
    "lstm1 = Flatten()(lstm1)\n",
    "\n",
    "lstm2 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(text_input)\n",
    "lstm2 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(lstm2)\n",
    "lstm2 = Flatten()(lstm2)\n",
    "\n",
    "lstms = concatenate([lstm1, lstm2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = Dense(64)(lstms)\n",
    "x1 = LeakyReLU()(x1)\n",
    "x1 = Dense(1, activation = 'linear', name='regression')(x1)\n",
    "\n",
    "x2 = Dense(64)(lstms)\n",
    "x2 = LeakyReLU()(x2)\n",
    "x2 = Dropout(0.9)(x2)\n",
    "x2 = Dense(1, activation = 'sigmoid', name = 'class')(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = Model(inputs=[main_input, text_input], \n",
    "              outputs=[x1, x2])\n",
    "opt = Nadam(lr=0.002, clipnorm = 0.5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=50, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(monitor='val_loss', filepath=\"model.hdf5\", verbose=1, save_best_only=True)\n",
    "final_model.compile(optimizer=opt, loss={'regression': 'mse', 'class': 'binary_crossentropy'}, loss_weights=[1., 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x129d2a278> (None, 30, 5)\n",
      "<keras.engine.topology.InputLayer object at 0x129d2a048> (None, 30, 100)\n",
      "<keras.layers.recurrent.LSTM object at 0x129d2a470> (None, 30, 10)\n",
      "<keras.layers.recurrent.LSTM object at 0x127c810f0> (None, 30, 10)\n",
      "<keras.layers.recurrent.LSTM object at 0x129d2a160> (None, 30, 10)\n",
      "<keras.layers.recurrent.LSTM object at 0x13897f080> (None, 30, 10)\n",
      "<keras.layers.core.Flatten object at 0x1242047b8> (None, 300)\n",
      "<keras.layers.core.Flatten object at 0x13889c2b0> (None, 300)\n",
      "<keras.layers.merge.Concatenate object at 0x11d16e240> (None, 600)\n",
      "<keras.layers.core.Dense object at 0x12d90f358> (None, 64)\n",
      "<keras.layers.core.Dense object at 0x12d90f518> (None, 64)\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x12292a780> (None, 64)\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x12d90f898> (None, 64)\n",
      "<keras.layers.core.Dropout object at 0x122946e80> (None, 64)\n",
      "<keras.layers.core.Dense object at 0x125574208> (None, 1)\n",
      "<keras.layers.core.Dense object at 0x1229467f0> (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in final_model.layers:\n",
    "    print(layer, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1580 samples, validate on 347 samples\n",
      "Epoch 1/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 1.0536 - regression_loss: 0.8704 - class_loss: 0.9163Epoch 00000: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 1.0317 - regression_loss: 0.8490 - class_loss: 0.9138 - val_loss: 1.1224 - val_regression_loss: 0.9278 - val_class_loss: 0.9729\n",
      "Epoch 2/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.6144 - regression_loss: 0.4439 - class_loss: 0.8525Epoch 00001: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.6169 - regression_loss: 0.4469 - class_loss: 0.8497 - val_loss: 0.2572 - val_regression_loss: 0.1173 - val_class_loss: 0.6991\n",
      "Epoch 3/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.4900 - regression_loss: 0.3429 - class_loss: 0.7355Epoch 00002: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.4827 - regression_loss: 0.3354 - class_loss: 0.7363 - val_loss: 0.6066 - val_regression_loss: 0.4659 - val_class_loss: 0.7039\n",
      "Epoch 4/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.4230 - regression_loss: 0.2754 - class_loss: 0.7376Epoch 00003: val_loss improved from 0.21150 to 0.18853, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.4258 - regression_loss: 0.2783 - class_loss: 0.7374 - val_loss: 0.1885 - val_regression_loss: 0.0499 - val_class_loss: 0.6934\n",
      "Epoch 5/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.3560 - regression_loss: 0.2111 - class_loss: 0.7245Epoch 00004: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.3510 - regression_loss: 0.2060 - class_loss: 0.7246 - val_loss: 0.4550 - val_regression_loss: 0.3155 - val_class_loss: 0.6971\n",
      "Epoch 6/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.3131 - regression_loss: 0.1683 - class_loss: 0.7238Epoch 00005: val_loss improved from 0.18853 to 0.16462, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.3141 - regression_loss: 0.1691 - class_loss: 0.7249 - val_loss: 0.1646 - val_regression_loss: 0.0246 - val_class_loss: 0.7001\n",
      "Epoch 7/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.2672 - regression_loss: 0.1236 - class_loss: 0.7180Epoch 00006: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.2691 - regression_loss: 0.1250 - class_loss: 0.7206 - val_loss: 0.2858 - val_regression_loss: 0.1451 - val_class_loss: 0.7036\n",
      "Epoch 8/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.2384 - regression_loss: 0.0941 - class_loss: 0.7216Epoch 00007: val_loss improved from 0.16462 to 0.15440, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.2388 - regression_loss: 0.0945 - class_loss: 0.7215 - val_loss: 0.1544 - val_regression_loss: 0.0156 - val_class_loss: 0.6941\n",
      "Epoch 9/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.2145 - regression_loss: 0.0710 - class_loss: 0.7175Epoch 00008: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.2140 - regression_loss: 0.0706 - class_loss: 0.7170 - val_loss: 0.1969 - val_regression_loss: 0.0562 - val_class_loss: 0.7035\n",
      "Epoch 10/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.2046 - regression_loss: 0.0584 - class_loss: 0.7312Epoch 00009: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.2034 - regression_loss: 0.0576 - class_loss: 0.7285 - val_loss: 0.1570 - val_regression_loss: 0.0149 - val_class_loss: 0.7106\n",
      "Epoch 11/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1934 - regression_loss: 0.0498 - class_loss: 0.7184Epoch 00010: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1932 - regression_loss: 0.0496 - class_loss: 0.7182 - val_loss: 0.1585 - val_regression_loss: 0.0198 - val_class_loss: 0.6936\n",
      "Epoch 12/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1865 - regression_loss: 0.0438 - class_loss: 0.7139Epoch 00011: val_loss improved from 0.15440 to 0.15136, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1855 - regression_loss: 0.0429 - class_loss: 0.7130 - val_loss: 0.1514 - val_regression_loss: 0.0123 - val_class_loss: 0.6954\n",
      "Epoch 13/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1768 - regression_loss: 0.0355 - class_loss: 0.7065Epoch 00012: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1800 - regression_loss: 0.0387 - class_loss: 0.7067 - val_loss: 0.1582 - val_regression_loss: 0.0195 - val_class_loss: 0.6936\n",
      "Epoch 14/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1851 - regression_loss: 0.0432 - class_loss: 0.7096Epoch 00013: val_loss improved from 0.15136 to 0.14970, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1840 - regression_loss: 0.0422 - class_loss: 0.7091 - val_loss: 0.1497 - val_regression_loss: 0.0110 - val_class_loss: 0.6934\n",
      "Epoch 15/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1800 - regression_loss: 0.0389 - class_loss: 0.7052Epoch 00014: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1793 - regression_loss: 0.0383 - class_loss: 0.7050 - val_loss: 0.1521 - val_regression_loss: 0.0124 - val_class_loss: 0.6985\n",
      "Epoch 16/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1787 - regression_loss: 0.0382 - class_loss: 0.7025Epoch 00015: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1780 - regression_loss: 0.0374 - class_loss: 0.7031 - val_loss: 0.1511 - val_regression_loss: 0.0116 - val_class_loss: 0.6976\n",
      "Epoch 17/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1778 - regression_loss: 0.0380 - class_loss: 0.6991Epoch 00016: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1768 - regression_loss: 0.0371 - class_loss: 0.6982 - val_loss: 0.1504 - val_regression_loss: 0.0108 - val_class_loss: 0.6981\n",
      "Epoch 18/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1783 - regression_loss: 0.0384 - class_loss: 0.6993Epoch 00017: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1775 - regression_loss: 0.0378 - class_loss: 0.6987 - val_loss: 0.1506 - val_regression_loss: 0.0114 - val_class_loss: 0.6961\n",
      "Epoch 19/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1797 - regression_loss: 0.0395 - class_loss: 0.7007Epoch 00018: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1787 - regression_loss: 0.0387 - class_loss: 0.6996 - val_loss: 0.1504 - val_regression_loss: 0.0112 - val_class_loss: 0.6959\n",
      "Epoch 20/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1785 - regression_loss: 0.0382 - class_loss: 0.7014Epoch 00019: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1775 - regression_loss: 0.0373 - class_loss: 0.7008 - val_loss: 0.1502 - val_regression_loss: 0.0107 - val_class_loss: 0.6976\n",
      "Epoch 21/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1782 - regression_loss: 0.0381 - class_loss: 0.7006Epoch 00020: val_loss improved from 0.14970 to 0.14954, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1772 - regression_loss: 0.0372 - class_loss: 0.7003 - val_loss: 0.1495 - val_regression_loss: 0.0107 - val_class_loss: 0.6940\n",
      "Epoch 22/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1771 - regression_loss: 0.0380 - class_loss: 0.6958Epoch 00021: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1763 - regression_loss: 0.0372 - class_loss: 0.6954 - val_loss: 0.1501 - val_regression_loss: 0.0109 - val_class_loss: 0.6960\n",
      "Epoch 23/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1778 - regression_loss: 0.0380 - class_loss: 0.6993Epoch 00022: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1769 - regression_loss: 0.0371 - class_loss: 0.6990 - val_loss: 0.1501 - val_regression_loss: 0.0107 - val_class_loss: 0.6972\n",
      "Epoch 24/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1771 - regression_loss: 0.0377 - class_loss: 0.6967Epoch 00023: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1765 - regression_loss: 0.0370 - class_loss: 0.6972 - val_loss: 0.1498 - val_regression_loss: 0.0110 - val_class_loss: 0.6939\n",
      "Epoch 25/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1770 - regression_loss: 0.0379 - class_loss: 0.6956Epoch 00024: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1760 - regression_loss: 0.0370 - class_loss: 0.6949 - val_loss: 0.1497 - val_regression_loss: 0.0107 - val_class_loss: 0.6951\n",
      "Epoch 26/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1768 - regression_loss: 0.0366 - class_loss: 0.7010Epoch 00025: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1770 - regression_loss: 0.0367 - class_loss: 0.7014 - val_loss: 0.1525 - val_regression_loss: 0.0135 - val_class_loss: 0.6952\n",
      "Epoch 27/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1762 - regression_loss: 0.0376 - class_loss: 0.6932Epoch 00026: val_loss improved from 0.14954 to 0.14951, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1755 - regression_loss: 0.0367 - class_loss: 0.6938 - val_loss: 0.1495 - val_regression_loss: 0.0105 - val_class_loss: 0.6950\n",
      "Epoch 28/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1763 - regression_loss: 0.0376 - class_loss: 0.6935Epoch 00027: val_loss improved from 0.14951 to 0.14931, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1755 - regression_loss: 0.0367 - class_loss: 0.6939 - val_loss: 0.1493 - val_regression_loss: 0.0104 - val_class_loss: 0.6947\n",
      "Epoch 29/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1658 - regression_loss: 0.0264 - class_loss: 0.6970Epoch 00028: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1756 - regression_loss: 0.0361 - class_loss: 0.6977 - val_loss: 0.1565 - val_regression_loss: 0.0176 - val_class_loss: 0.6949\n",
      "Epoch 30/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1769 - regression_loss: 0.0388 - class_loss: 0.6902Epoch 00029: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1759 - regression_loss: 0.0379 - class_loss: 0.6899 - val_loss: 0.1497 - val_regression_loss: 0.0102 - val_class_loss: 0.6972\n",
      "Epoch 31/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1747 - regression_loss: 0.0360 - class_loss: 0.6936Epoch 00030: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1738 - regression_loss: 0.0351 - class_loss: 0.6932 - val_loss: 0.1499 - val_regression_loss: 0.0105 - val_class_loss: 0.6973\n",
      "Epoch 32/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1764 - regression_loss: 0.0367 - class_loss: 0.6988Epoch 00031: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1758 - regression_loss: 0.0360 - class_loss: 0.6990 - val_loss: 0.1495 - val_regression_loss: 0.0104 - val_class_loss: 0.6956\n",
      "Epoch 33/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1726 - regression_loss: 0.0341 - class_loss: 0.6922Epoch 00032: val_loss improved from 0.14931 to 0.14867, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1718 - regression_loss: 0.0334 - class_loss: 0.6919 - val_loss: 0.1487 - val_regression_loss: 0.0097 - val_class_loss: 0.6949\n",
      "Epoch 34/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1745 - regression_loss: 0.0355 - class_loss: 0.6951Epoch 00033: val_loss improved from 0.14867 to 0.14833, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1738 - regression_loss: 0.0348 - class_loss: 0.6952 - val_loss: 0.1483 - val_regression_loss: 0.0093 - val_class_loss: 0.6951\n",
      "Epoch 35/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1708 - regression_loss: 0.0324 - class_loss: 0.6921Epoch 00034: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1700 - regression_loss: 0.0316 - class_loss: 0.6917 - val_loss: 0.1486 - val_regression_loss: 0.0094 - val_class_loss: 0.6962\n",
      "Epoch 36/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1618 - regression_loss: 0.0235 - class_loss: 0.6913Epoch 00035: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1713 - regression_loss: 0.0330 - class_loss: 0.6914 - val_loss: 0.1545 - val_regression_loss: 0.0153 - val_class_loss: 0.6958\n",
      "Epoch 37/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1727 - regression_loss: 0.0341 - class_loss: 0.6931Epoch 00036: val_loss improved from 0.14833 to 0.14758, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1722 - regression_loss: 0.0336 - class_loss: 0.6929 - val_loss: 0.1476 - val_regression_loss: 0.0086 - val_class_loss: 0.6949\n",
      "Epoch 38/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1702 - regression_loss: 0.0327 - class_loss: 0.6875Epoch 00037: val_loss improved from 0.14758 to 0.14656, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1697 - regression_loss: 0.0320 - class_loss: 0.6885 - val_loss: 0.1466 - val_regression_loss: 0.0077 - val_class_loss: 0.6943\n",
      "Epoch 39/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1677 - regression_loss: 0.0282 - class_loss: 0.6974Epoch 00038: val_loss improved from 0.14656 to 0.14597, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1678 - regression_loss: 0.0282 - class_loss: 0.6977 - val_loss: 0.1460 - val_regression_loss: 0.0072 - val_class_loss: 0.6941\n",
      "Epoch 40/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1666 - regression_loss: 0.0281 - class_loss: 0.6929Epoch 00039: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1661 - regression_loss: 0.0275 - class_loss: 0.6930 - val_loss: 0.1467 - val_regression_loss: 0.0078 - val_class_loss: 0.6945\n",
      "Epoch 41/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1726 - regression_loss: 0.0342 - class_loss: 0.6923Epoch 00040: val_loss improved from 0.14597 to 0.14548, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1718 - regression_loss: 0.0334 - class_loss: 0.6918 - val_loss: 0.1455 - val_regression_loss: 0.0065 - val_class_loss: 0.6950\n",
      "Epoch 42/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1679 - regression_loss: 0.0293 - class_loss: 0.6929Epoch 00041: val_loss improved from 0.14548 to 0.14469, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1674 - regression_loss: 0.0287 - class_loss: 0.6935 - val_loss: 0.1447 - val_regression_loss: 0.0058 - val_class_loss: 0.6946\n",
      "Epoch 43/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1660 - regression_loss: 0.0277 - class_loss: 0.6914Epoch 00042: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1656 - regression_loss: 0.0273 - class_loss: 0.6918 - val_loss: 0.1450 - val_regression_loss: 0.0061 - val_class_loss: 0.6944\n",
      "Epoch 44/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1657 - regression_loss: 0.0269 - class_loss: 0.6940Epoch 00043: val_loss improved from 0.14469 to 0.14415, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1650 - regression_loss: 0.0262 - class_loss: 0.6940 - val_loss: 0.1442 - val_regression_loss: 0.0052 - val_class_loss: 0.6947\n",
      "Epoch 45/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1594 - regression_loss: 0.0209 - class_loss: 0.6924Epoch 00044: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1591 - regression_loss: 0.0207 - class_loss: 0.6921 - val_loss: 0.1442 - val_regression_loss: 0.0053 - val_class_loss: 0.6944\n",
      "Epoch 46/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1636 - regression_loss: 0.0251 - class_loss: 0.6925Epoch 00045: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1629 - regression_loss: 0.0245 - class_loss: 0.6924 - val_loss: 0.1442 - val_regression_loss: 0.0054 - val_class_loss: 0.6942\n",
      "Epoch 47/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1571 - regression_loss: 0.0190 - class_loss: 0.6908Epoch 00046: val_loss improved from 0.14415 to 0.14409, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1572 - regression_loss: 0.0191 - class_loss: 0.6906 - val_loss: 0.1441 - val_regression_loss: 0.0051 - val_class_loss: 0.6951\n",
      "Epoch 48/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1602 - regression_loss: 0.0222 - class_loss: 0.6900Epoch 00047: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1598 - regression_loss: 0.0217 - class_loss: 0.6902 - val_loss: 0.1452 - val_regression_loss: 0.0060 - val_class_loss: 0.6960\n",
      "Epoch 49/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1538 - regression_loss: 0.0153 - class_loss: 0.6926Epoch 00048: val_loss improved from 0.14409 to 0.14373, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1535 - regression_loss: 0.0150 - class_loss: 0.6925 - val_loss: 0.1437 - val_regression_loss: 0.0047 - val_class_loss: 0.6952\n",
      "Epoch 50/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1549 - regression_loss: 0.0168 - class_loss: 0.6906Epoch 00049: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1547 - regression_loss: 0.0165 - class_loss: 0.6909 - val_loss: 0.1442 - val_regression_loss: 0.0052 - val_class_loss: 0.6949\n",
      "Epoch 51/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1526 - regression_loss: 0.0150 - class_loss: 0.6878Epoch 00050: val_loss improved from 0.14373 to 0.14326, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1528 - regression_loss: 0.0152 - class_loss: 0.6882 - val_loss: 0.1433 - val_regression_loss: 0.0043 - val_class_loss: 0.6947\n",
      "Epoch 52/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1596 - regression_loss: 0.0211 - class_loss: 0.6926Epoch 00051: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1592 - regression_loss: 0.0206 - class_loss: 0.6929 - val_loss: 0.1437 - val_regression_loss: 0.0047 - val_class_loss: 0.6948\n",
      "Epoch 53/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1593 - regression_loss: 0.0210 - class_loss: 0.6918Epoch 00052: val_loss improved from 0.14326 to 0.14299, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1589 - regression_loss: 0.0205 - class_loss: 0.6918 - val_loss: 0.1430 - val_regression_loss: 0.0041 - val_class_loss: 0.6942\n",
      "Epoch 54/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1633 - regression_loss: 0.0254 - class_loss: 0.6897Epoch 00053: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1626 - regression_loss: 0.0248 - class_loss: 0.6891 - val_loss: 0.1432 - val_regression_loss: 0.0041 - val_class_loss: 0.6954\n",
      "Epoch 55/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1577 - regression_loss: 0.0191 - class_loss: 0.6933Epoch 00054: val_loss improved from 0.14299 to 0.14268, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1574 - regression_loss: 0.0189 - class_loss: 0.6926 - val_loss: 0.1427 - val_regression_loss: 0.0036 - val_class_loss: 0.6956\n",
      "Epoch 56/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1504 - regression_loss: 0.0120 - class_loss: 0.6916Epoch 00055: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1502 - regression_loss: 0.0118 - class_loss: 0.6918 - val_loss: 0.1430 - val_regression_loss: 0.0040 - val_class_loss: 0.6952\n",
      "Epoch 57/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1524 - regression_loss: 0.0141 - class_loss: 0.6917Epoch 00056: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1525 - regression_loss: 0.0141 - class_loss: 0.6918 - val_loss: 0.1438 - val_regression_loss: 0.0047 - val_class_loss: 0.6953\n",
      "Epoch 58/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1553 - regression_loss: 0.0176 - class_loss: 0.6883Epoch 00057: val_loss improved from 0.14268 to 0.14244, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1550 - regression_loss: 0.0172 - class_loss: 0.6887 - val_loss: 0.1424 - val_regression_loss: 0.0034 - val_class_loss: 0.6951\n",
      "Epoch 59/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1622 - regression_loss: 0.0243 - class_loss: 0.6891Epoch 00058: val_loss improved from 0.14244 to 0.14225, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1616 - regression_loss: 0.0237 - class_loss: 0.6894 - val_loss: 0.1423 - val_regression_loss: 0.0032 - val_class_loss: 0.6950\n",
      "Epoch 60/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1540 - regression_loss: 0.0156 - class_loss: 0.6918Epoch 00059: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1537 - regression_loss: 0.0153 - class_loss: 0.6919 - val_loss: 0.1423 - val_regression_loss: 0.0033 - val_class_loss: 0.6953\n",
      "Epoch 61/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1567 - regression_loss: 0.0186 - class_loss: 0.6902Epoch 00060: val_loss improved from 0.14225 to 0.14216, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1563 - regression_loss: 0.0182 - class_loss: 0.6903 - val_loss: 0.1422 - val_regression_loss: 0.0031 - val_class_loss: 0.6953\n",
      "Epoch 62/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1467 - regression_loss: 0.0084 - class_loss: 0.6918Epoch 00061: val_loss improved from 0.14216 to 0.14210, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1467 - regression_loss: 0.0083 - class_loss: 0.6916 - val_loss: 0.1421 - val_regression_loss: 0.0030 - val_class_loss: 0.6955\n",
      "Epoch 63/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1542 - regression_loss: 0.0158 - class_loss: 0.6921Epoch 00062: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1542 - regression_loss: 0.0159 - class_loss: 0.6917 - val_loss: 0.1424 - val_regression_loss: 0.0034 - val_class_loss: 0.6952\n",
      "Epoch 64/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1535 - regression_loss: 0.0155 - class_loss: 0.6899Epoch 00063: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1535 - regression_loss: 0.0154 - class_loss: 0.6905 - val_loss: 0.1444 - val_regression_loss: 0.0055 - val_class_loss: 0.6946\n",
      "Epoch 65/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1538 - regression_loss: 0.0159 - class_loss: 0.6894Epoch 00064: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1535 - regression_loss: 0.0155 - class_loss: 0.6898 - val_loss: 0.1422 - val_regression_loss: 0.0033 - val_class_loss: 0.6946\n",
      "Epoch 66/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1467 - regression_loss: 0.0081 - class_loss: 0.6929Epoch 00065: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1467 - regression_loss: 0.0081 - class_loss: 0.6931 - val_loss: 0.1425 - val_regression_loss: 0.0036 - val_class_loss: 0.6944\n",
      "Epoch 67/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1470 - regression_loss: 0.0089 - class_loss: 0.6907Epoch 00066: val_loss improved from 0.14210 to 0.14195, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1469 - regression_loss: 0.0087 - class_loss: 0.6907 - val_loss: 0.1419 - val_regression_loss: 0.0030 - val_class_loss: 0.6945\n",
      "Epoch 68/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1473 - regression_loss: 0.0094 - class_loss: 0.6896Epoch 00067: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1471 - regression_loss: 0.0092 - class_loss: 0.6892 - val_loss: 0.1422 - val_regression_loss: 0.0032 - val_class_loss: 0.6950\n",
      "Epoch 69/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1556 - regression_loss: 0.0172 - class_loss: 0.6918Epoch 00068: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1553 - regression_loss: 0.0169 - class_loss: 0.6920 - val_loss: 0.1422 - val_regression_loss: 0.0032 - val_class_loss: 0.6949\n",
      "Epoch 70/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1542 - regression_loss: 0.0161 - class_loss: 0.6904Epoch 00069: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1566 - regression_loss: 0.0185 - class_loss: 0.6907 - val_loss: 0.1474 - val_regression_loss: 0.0085 - val_class_loss: 0.6947\n",
      "Epoch 71/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1506 - regression_loss: 0.0125 - class_loss: 0.6905Epoch 00070: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1504 - regression_loss: 0.0123 - class_loss: 0.6904 - val_loss: 0.1421 - val_regression_loss: 0.0032 - val_class_loss: 0.6949\n",
      "Epoch 72/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1468 - regression_loss: 0.0083 - class_loss: 0.6926Epoch 00071: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1468 - regression_loss: 0.0083 - class_loss: 0.6927 - val_loss: 0.1424 - val_regression_loss: 0.0034 - val_class_loss: 0.6951\n",
      "Epoch 73/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1471 - regression_loss: 0.0091 - class_loss: 0.6896Epoch 00072: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1469 - regression_loss: 0.0090 - class_loss: 0.6897 - val_loss: 0.1420 - val_regression_loss: 0.0030 - val_class_loss: 0.6950\n",
      "Epoch 74/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1545 - regression_loss: 0.0166 - class_loss: 0.6895Epoch 00073: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1543 - regression_loss: 0.0163 - class_loss: 0.6899 - val_loss: 0.1427 - val_regression_loss: 0.0037 - val_class_loss: 0.6949\n",
      "Epoch 75/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1447 - regression_loss: 0.0066 - class_loss: 0.6905Epoch 00074: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1498 - regression_loss: 0.0118 - class_loss: 0.6903 - val_loss: 0.1447 - val_regression_loss: 0.0057 - val_class_loss: 0.6950\n",
      "Epoch 76/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1548 - regression_loss: 0.0168 - class_loss: 0.6902Epoch 00075: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1546 - regression_loss: 0.0164 - class_loss: 0.6908 - val_loss: 0.1426 - val_regression_loss: 0.0035 - val_class_loss: 0.6952\n",
      "Epoch 77/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1443 - regression_loss: 0.0063 - class_loss: 0.6902Epoch 00076: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1442 - regression_loss: 0.0062 - class_loss: 0.6900 - val_loss: 0.1426 - val_regression_loss: 0.0035 - val_class_loss: 0.6953\n",
      "Epoch 78/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1503 - regression_loss: 0.0122 - class_loss: 0.6905Epoch 00077: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1505 - regression_loss: 0.0123 - class_loss: 0.6909 - val_loss: 0.1423 - val_regression_loss: 0.0032 - val_class_loss: 0.6955\n",
      "Epoch 79/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1471 - regression_loss: 0.0092 - class_loss: 0.6896Epoch 00078: val_loss improved from 0.14195 to 0.14195, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1470 - regression_loss: 0.0091 - class_loss: 0.6899 - val_loss: 0.1419 - val_regression_loss: 0.0030 - val_class_loss: 0.6950\n",
      "Epoch 80/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1483 - regression_loss: 0.0103 - class_loss: 0.6899Epoch 00079: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1481 - regression_loss: 0.0101 - class_loss: 0.6899 - val_loss: 0.1420 - val_regression_loss: 0.0030 - val_class_loss: 0.6948\n",
      "Epoch 81/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1450 - regression_loss: 0.0068 - class_loss: 0.6907Epoch 00080: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1449 - regression_loss: 0.0068 - class_loss: 0.6906 - val_loss: 0.1424 - val_regression_loss: 0.0034 - val_class_loss: 0.6949\n",
      "Epoch 82/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1486 - regression_loss: 0.0104 - class_loss: 0.6906Epoch 00081: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1488 - regression_loss: 0.0107 - class_loss: 0.6908 - val_loss: 0.1439 - val_regression_loss: 0.0049 - val_class_loss: 0.6949\n",
      "Epoch 83/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1477 - regression_loss: 0.0094 - class_loss: 0.6913Epoch 00082: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1476 - regression_loss: 0.0094 - class_loss: 0.6914 - val_loss: 0.1420 - val_regression_loss: 0.0030 - val_class_loss: 0.6951\n",
      "Epoch 84/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1522 - regression_loss: 0.0143 - class_loss: 0.6893Epoch 00083: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1519 - regression_loss: 0.0140 - class_loss: 0.6891 - val_loss: 0.1420 - val_regression_loss: 0.0028 - val_class_loss: 0.6956\n",
      "Epoch 85/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1540 - regression_loss: 0.0156 - class_loss: 0.6920Epoch 00084: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1535 - regression_loss: 0.0152 - class_loss: 0.6913 - val_loss: 0.1420 - val_regression_loss: 0.0028 - val_class_loss: 0.6958\n",
      "Epoch 86/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1449 - regression_loss: 0.0066 - class_loss: 0.6914Epoch 00085: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1448 - regression_loss: 0.0066 - class_loss: 0.6913 - val_loss: 0.1423 - val_regression_loss: 0.0032 - val_class_loss: 0.6956\n",
      "Epoch 87/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1454 - regression_loss: 0.0070 - class_loss: 0.6922Epoch 00086: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1453 - regression_loss: 0.0069 - class_loss: 0.6924 - val_loss: 0.1421 - val_regression_loss: 0.0031 - val_class_loss: 0.6953\n",
      "Epoch 88/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1458 - regression_loss: 0.0081 - class_loss: 0.6883Epoch 00087: val_loss improved from 0.14195 to 0.14185, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1459 - regression_loss: 0.0082 - class_loss: 0.6886 - val_loss: 0.1419 - val_regression_loss: 0.0028 - val_class_loss: 0.6952\n",
      "Epoch 89/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1445 - regression_loss: 0.0065 - class_loss: 0.6897Epoch 00088: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1444 - regression_loss: 0.0064 - class_loss: 0.6900 - val_loss: 0.1419 - val_regression_loss: 0.0029 - val_class_loss: 0.6951\n",
      "Epoch 90/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1474 - regression_loss: 0.0095 - class_loss: 0.6895Epoch 00089: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1480 - regression_loss: 0.0099 - class_loss: 0.6903 - val_loss: 0.1420 - val_regression_loss: 0.0030 - val_class_loss: 0.6948\n",
      "Epoch 91/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1485 - regression_loss: 0.0108 - class_loss: 0.6884Epoch 00090: val_loss improved from 0.14185 to 0.14169, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1484 - regression_loss: 0.0106 - class_loss: 0.6890 - val_loss: 0.1417 - val_regression_loss: 0.0027 - val_class_loss: 0.6947\n",
      "Epoch 92/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1451 - regression_loss: 0.0072 - class_loss: 0.6896Epoch 00091: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1451 - regression_loss: 0.0072 - class_loss: 0.6894 - val_loss: 0.1427 - val_regression_loss: 0.0037 - val_class_loss: 0.6948\n",
      "Epoch 93/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1505 - regression_loss: 0.0123 - class_loss: 0.6911Epoch 00092: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1504 - regression_loss: 0.0122 - class_loss: 0.6911 - val_loss: 0.1422 - val_regression_loss: 0.0032 - val_class_loss: 0.6952\n",
      "Epoch 94/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1507 - regression_loss: 0.0128 - class_loss: 0.6895Epoch 00093: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1506 - regression_loss: 0.0126 - class_loss: 0.6901 - val_loss: 0.1417 - val_regression_loss: 0.0027 - val_class_loss: 0.6950\n",
      "Epoch 95/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1523 - regression_loss: 0.0143 - class_loss: 0.6898Epoch 00094: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1525 - regression_loss: 0.0145 - class_loss: 0.6900 - val_loss: 0.1422 - val_regression_loss: 0.0032 - val_class_loss: 0.6951\n",
      "Epoch 96/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1528 - regression_loss: 0.0147 - class_loss: 0.6906Epoch 00095: val_loss improved from 0.14169 to 0.14164, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1526 - regression_loss: 0.0144 - class_loss: 0.6908 - val_loss: 0.1416 - val_regression_loss: 0.0027 - val_class_loss: 0.6949\n",
      "Epoch 97/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1432 - regression_loss: 0.0052 - class_loss: 0.6903Epoch 00096: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1432 - regression_loss: 0.0052 - class_loss: 0.6903 - val_loss: 0.1419 - val_regression_loss: 0.0028 - val_class_loss: 0.6951\n",
      "Epoch 98/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1429 - regression_loss: 0.0048 - class_loss: 0.6902Epoch 00097: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1428 - regression_loss: 0.0048 - class_loss: 0.6898 - val_loss: 0.1417 - val_regression_loss: 0.0026 - val_class_loss: 0.6952\n",
      "Epoch 99/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1504 - regression_loss: 0.0123 - class_loss: 0.6906Epoch 00098: val_loss improved from 0.14164 to 0.14147, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1503 - regression_loss: 0.0122 - class_loss: 0.6906 - val_loss: 0.1415 - val_regression_loss: 0.0024 - val_class_loss: 0.6954\n",
      "Epoch 100/100\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1439 - regression_loss: 0.0057 - class_loss: 0.6907Epoch 00099: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1438 - regression_loss: 0.0057 - class_loss: 0.6908 - val_loss: 0.1415 - val_regression_loss: 0.0024 - val_class_loss: 0.6953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFEREpEKhICIiFQoFERGp+P8BI6ZhUC42h/wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1382bea58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = final_model.fit([X_train, X_train_text], [Y_train, Y_train2],\n",
    "        nb_epoch = 100, \n",
    "        batch_size = 256, \n",
    "        verbose=1, \n",
    "        validation_data=([X_test, X_test_text], [Y_test, Y_test2]), \n",
    "        callbacks=[reduce_lr, checkpointer], shuffle=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1383d0a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0188202172326\n",
      "0.0729423060054\n",
      "3.32715254069\n"
     ]
    }
   ],
   "source": [
    "final_model.load_weights(\"model.hdf5\")\n",
    "pred = final_model.predict([X_test, X_test_text])[0]\n",
    "\n",
    "predicted = pred\n",
    "original = Y_test\n",
    "\n",
    "plt.title('Actual and predicted')\n",
    "plt.legend(loc='best')\n",
    "plt.plot(original, color='black', label = 'Original data')\n",
    "plt.plot(pred, color='blue', label = 'Predicted data')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(np.mean(np.square(predicted - original)))\n",
    "print(np.mean(np.abs(predicted - original)))\n",
    "print(np.mean(np.abs((original - predicted) / original)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
