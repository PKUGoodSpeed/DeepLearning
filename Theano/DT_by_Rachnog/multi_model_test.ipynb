{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../csv_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten, Permute\n",
    "from keras.layers import Merge, Input, concatenate, average, add\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D, RepeatVector, AveragePooling1D\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras import regularizers\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
    "from keras.initializers import *\n",
    "from keras.constraints import *\n",
    "from keras import regularizers\n",
    "from keras import losses\n",
    "\n",
    "#Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import date\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Functions for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_text_csv(filename = 'multimodel/Combined_News_DJIA.csv', date_split = date(2014,12,31)):\n",
    "    '''\n",
    "    Load news from csv, group them and split in train/test set due to @date_split\n",
    "    '''\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Combined']=df.iloc[:,2:27].apply(lambda row: ''.join(str(row.values)), axis=1)\n",
    "    \n",
    "    train = df.loc[(pd.to_datetime(df[\"Date\"]) <= date_split),['Label','Combined']]\n",
    "    test = df.loc[(pd.to_datetime(df[\"Date\"]) > date_split),['Label','Combined']]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 378\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "      Label                                           Combined\n",
      "1611      1  [ 'Most cases of cancer are the result of shee...\n",
      "1612      0  [ 'Moscow-&gt;Beijing high speed train will re...\n",
      "1613      0  ['US oil falls below $50 a barrel'\\n \"Toyota g...\n",
      "1614      1  [\"'Shots fired' at French magazine HQ\"\\n '90% ...\n",
      "1615      1  [ 'New Charlie Hebdo issue to come out next we...\n"
     ]
    }
   ],
   "source": [
    "'''Using news as a input!!'''\n",
    "train, test = load_text_csv()\n",
    "print(len(train), len(test))\n",
    "print(type(test))\n",
    "print(test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data2change(data):\n",
    "    change = pd.DataFrame(data).pct_change()\n",
    "    change = change.replace([np.inf, -np.inf], np.nan)\n",
    "    change = change.fillna(0.).values.tolist()\n",
    "    change = [c[0] for c in change]\n",
    "    return change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ts_csv(filename = 'multimodel/DJIA_table.csv', date_split = date(2014,12,31)):\n",
    "    '''\n",
    "    Load time series from csv, taking adjustment close prices;\n",
    "    transforming them into percentage of price change;\n",
    "    split in train/test set due to @date_split\n",
    "    '''\n",
    "    \n",
    "    data_original = pd.read_csv(filename)[::-1]\n",
    "    \n",
    "    train2 = data_original.loc[(pd.to_datetime(data_original[\"Date\"]) <= date_split)]\n",
    "    test2 = data_original.loc[(pd.to_datetime(data_original[\"Date\"]) > date_split)]\n",
    "    \n",
    "    open_train = train2.loc[:, 'Open']\n",
    "    open_test = test2.loc[:, 'Open']\n",
    "    \n",
    "    high_train = train2.loc[:, 'High']\n",
    "    high_test = test2.loc[:, 'High']\n",
    "    \n",
    "    low_train = train2.loc[:, 'Low']\n",
    "    low_test = test2.loc[:, 'Low']\n",
    "    \n",
    "    close_train = train2.loc[:, 'Close']\n",
    "    close_test = test2.loc[:, 'Close']\n",
    "    \n",
    "    volume_train = train2.loc[:, 'Volume']\n",
    "    volume_test = test2.loc[:, 'Volume']\n",
    "    \n",
    "    open_train = data2change(open_train)\n",
    "    open_test = data2change(open_test)\n",
    "    \n",
    "    high_train = data2change(high_train)\n",
    "    high_test = data2change(high_test)\n",
    "    \n",
    "    low_train = data2change(low_train)\n",
    "    low_test = data2change(low_test)\n",
    "    \n",
    "    close_test = data2change(close_test)\n",
    "    close_train = data2change(close_train)\n",
    "    \n",
    "    volume_train = data2change(volume_train)\n",
    "    volume_test = data2change(volume_test)\n",
    "    \n",
    "    train = np.column_stack((open_train, high_train, low_train, close_train, volume_train))\n",
    "    test = np.column_stack((open_test, high_test, low_test, close_test, volume_test))\n",
    "    \n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1611, 5)\n",
      "(378, 5)\n",
      "1611 378\n",
      "[[  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [ -9.92831745e-05  -7.26827764e-03  -1.44022763e-02  -1.85801617e-02\n",
      "    5.23010358e-01]\n",
      " [ -1.77944975e-02  -1.34810586e-02  -1.22202682e-02  -7.42842893e-03\n",
      "   -1.23019972e-01]\n",
      " [ -7.39254243e-03   9.11737142e-04   6.51186144e-03   1.22543927e-02\n",
      "   -1.06410131e-01]\n",
      " [  1.25003836e-02   1.81256767e-02   1.25003836e-02   1.83883108e-02\n",
      "    2.62111392e-01]]\n"
     ]
    }
   ],
   "source": [
    "'''[open price, high price, low price, close price, volume]'''\n",
    "data_chng_train, data_chng_test = load_ts_csv()\n",
    "print(len(data_chng_train), len(data_chng_test))\n",
    "print(data_chng_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_text_into_vectors(train_text, test_text, embedding_size = 100, model_path = 'word2vec10.model'):\n",
    "    '''\n",
    "        Transforms sentences into sequences of word2vec vectors\n",
    "        Returns train, test set and trained word2vec model\n",
    "    '''\n",
    "    data_for_w2v = []\n",
    "    for text in train_text + test_text:\n",
    "        words = text.split(' ')\n",
    "        data_for_w2v.append(words)\n",
    "\n",
    "    model = Word2Vec(data_for_w2v, size=embedding_size, window=5, min_count=1, workers=4)\n",
    "    model.save(model_path)\n",
    "    model = Word2Vec.load(model_path)\n",
    "\n",
    "    train_text_vectors = [[model[x] for x in sentence.split(' ')] for sentence in train_text]\n",
    "    test_text_vectors = [[model[x] for x in sentence.split(' ')] for sentence in test_text]\n",
    "\n",
    "    train_text_vectors = [np.mean(x, axis=0) for x in train_text_vectors]\n",
    "    test_text_vectors = [np.mean(x, axis=0) for x in test_text_vectors]\n",
    "\n",
    "    return train_text_vectors, test_text_vectors, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 378\n",
      "['case cancer result sheer bad luck rather unhealthi lifestyl diet even inherit gene new research suggest random mutat occur dna cell divid respons two third adult cancer across wide rang tissu iran dismiss unit state effort fight islam state ploy advanc u polici region realiti unit state act elimin daesh even interest weaken daesh interest manag poll one 8 german would join anti muslim march uk royal famili princ andrew name us lawsuit underag sex alleg 40 asylum seeker refus leav bu arriv destin rural northern sweden demand taken back malm big citi pakistani boat blow self india navi chase four peopl board vessel near pakistani port citi karachi believ kill dramat episod arabian sea new year eve accord india defenc ministri sweden hit third mosqu arson attack week 940 car set alight french new year salari top ceo rose twice fast averag canadian sinc recess studi norway violat equal pay law judg say judg find consul employe unjustli paid 30 000 less male counterpart imam want radic recruit muslim youth canada identifi dealt saudi arabia behead 83 peopl 2014 year live hell slave remot south korean island slaveri thrive chain rural island south korea rug southwest coast nurtur long histori exploit demand tri squeez live sea world 400 richest get richer ad 92bn 2014 rental car stereo infring copyright music right group say ukrainian minist threaten tv channel closur air russian entertain palestinian presid mahmoud abba enter seriou confront yet israel sign onto intern crimin court decis wednesday give court jurisdict crime commit palestinian land isra secur center publish name 50 kill terrorist conceal hama year 2014 deadliest year yet syria four year conflict 76 000 kill secret underground complex built nazi may use develop wmd includ nuclear bomb uncov austria restrict web freedom major global issu 2015 austrian journalist erich mchel deliv present hamburg annual meet chao comput club monday decemb 29 detail variou locat us nsa activ collect process electron intellig vienna thousand ukrain nationalist march kiev china new year resolut harvest execut prison organ author pull plug russia last polit independ tv station']\n"
     ]
    }
   ],
   "source": [
    "train_text = cPickle.load(open('multimodel/train_text.p', 'rb'))[:]\n",
    "test_text = cPickle.load(open('multimodel/test_text.p', 'rb'))[:]\n",
    "print(len(train_text), len(test_text))\n",
    "print(test_text[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1611 378\n",
      "<class 'gensim.models.word2vec.Word2Vec'>\n",
      "[array([-0.14618328,  0.28018647, -0.11567949,  0.20445451, -0.06263129,\n",
      "       -0.58806223,  0.51340634,  0.12562019,  0.90938127,  0.29875743,\n",
      "       -0.12280105,  0.18193203,  0.39279732,  0.17284322, -0.34758347,\n",
      "       -0.1607479 ,  0.64524204,  0.00920134, -0.11503026,  0.30297858,\n",
      "       -0.25572222, -0.14146295,  0.15011029, -0.16315621,  0.15668334,\n",
      "       -0.12726909,  0.020656  ,  0.20854218, -0.31821805, -0.31220305,\n",
      "       -0.2406311 ,  0.57030755,  0.06269305, -0.09370325, -0.17875282,\n",
      "        0.80828243, -0.17244528, -0.73326385, -0.19246346,  0.35881442,\n",
      "       -0.78913337, -0.14319178,  0.19593322, -0.0387511 , -0.06091733,\n",
      "       -0.50569707,  0.20446149, -0.11487626,  0.29574272, -0.15226474,\n",
      "        0.26182961, -0.61349636,  0.12472575,  0.37200141,  0.22627194,\n",
      "        0.25156137, -0.15251674,  0.39985812, -0.3696413 ,  0.61377263,\n",
      "        0.02237097,  0.44212544, -0.14337943, -0.17314142, -0.20484415,\n",
      "        0.04215064, -0.00921534, -0.18972076, -0.3384977 ,  0.25333473,\n",
      "        0.29111439, -0.53732955, -0.39423624, -0.13607003,  0.03646692,\n",
      "        0.30767655,  0.32021388,  0.18985713, -0.19283201, -0.31134537,\n",
      "       -0.23699383, -0.21911338, -0.37789184, -0.29437691, -0.1366092 ,\n",
      "        0.22073659, -0.21359622, -0.25990644, -0.08418278, -0.60880548,\n",
      "       -0.1194566 , -0.339073  ,  0.01453603, -0.35030606, -0.23236047,\n",
      "        0.1834179 , -0.14101043,  0.27259719, -0.05505111,  0.22181897], dtype=float32), array([-0.17185125,  0.30402252, -0.12071748,  0.24665122, -0.09522688,\n",
      "       -0.56909502,  0.52233505,  0.11909267,  0.93055636,  0.28471625,\n",
      "       -0.12981352,  0.19994198,  0.45542228,  0.15931521, -0.39068243,\n",
      "       -0.17704047,  0.66605097,  0.03143856, -0.10171174,  0.28713733,\n",
      "       -0.21582411, -0.10954595,  0.1655989 , -0.14259852,  0.2146107 ,\n",
      "       -0.07389085,  0.00384795,  0.19548759, -0.34323308, -0.30780205,\n",
      "       -0.22450922,  0.62185264,  0.0717704 , -0.11688236, -0.21439937,\n",
      "        0.79062557, -0.18082233, -0.72290111, -0.16585045,  0.37963998,\n",
      "       -0.76458472, -0.07229665,  0.21851097,  0.00147351, -0.06230418,\n",
      "       -0.52545011,  0.22915082, -0.18070333,  0.30708787, -0.14954209,\n",
      "        0.26325843, -0.63741034,  0.10108034,  0.299303  ,  0.2786262 ,\n",
      "        0.28704   , -0.18553431,  0.39632121, -0.38327989,  0.57934684,\n",
      "        0.08548062,  0.47333258, -0.1204327 , -0.12045899, -0.26161095,\n",
      "        0.09685704, -0.00925613, -0.18159656, -0.37776592,  0.2576158 ,\n",
      "        0.36180326, -0.59158725, -0.41387707, -0.13553233,  0.07298453,\n",
      "        0.3064326 ,  0.36574268,  0.1673279 , -0.20861255, -0.38371289,\n",
      "       -0.21553493, -0.23606753, -0.40561092, -0.28184247, -0.10841871,\n",
      "        0.23083706, -0.20521981, -0.18140125, -0.09979759, -0.65938634,\n",
      "       -0.13581395, -0.33248627,  0.0143732 , -0.40100309, -0.25082749,\n",
      "        0.20308614, -0.05946402,  0.33200645, -0.04605771,  0.24574356], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "train_text_vectors, test_text_vectors, model = transform_text_into_vectors(train_text, test_text, 100)\n",
    "print(len(train_text_vectors), len(test_text_vectors))\n",
    "print(type(model))\n",
    "print(test_text_vectors[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_into_XY(data_chng_train, train_text_vectors, step, window, forecast):\n",
    "    '''\n",
    "        Splits textual and time series data into train or test dataset for hybrid model;\n",
    "        objective y_i is percentage change of price movement for next day\n",
    "    '''\n",
    "    X_train, X_train_text, Y_train, Y_train2 = [], [], [], []\n",
    "    for i in range(0, len(data_chng_train), step): \n",
    "        try:\n",
    "            x_i = data_chng_train[i:i+window]\n",
    "            y_i = np.std(data_chng_train[i:i+window+forecast][3])\n",
    "\n",
    "            text_average = train_text_vectors[i:i+window]\n",
    "            last_close = x_i[-1]\n",
    "            y_i2 = None\n",
    "            if data_chng_train[i+window+forecast][3] > 0.:\n",
    "                y_i2 = 1.\n",
    "            else:\n",
    "                y_i2 = 0.\n",
    "\n",
    "        except Exception as e:\n",
    "            print('KEK', e)\n",
    "            break\n",
    "\n",
    "        X_train.append(x_i)\n",
    "        X_train_text.append(text_average)\n",
    "        Y_train.append(y_i)\n",
    "        Y_train2.append(y_i2)\n",
    "\n",
    "    X_train, X_train_text, Y_train, Y_train2 = np.array(X_train), np.array(X_train_text), np.array(Y_train), np.array(Y_train2)\n",
    "    return X_train, X_train_text, Y_train, Y_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEK index 1611 is out of bounds for axis 0 with size 1611\n",
      "KEK index 378 is out of bounds for axis 0 with size 378\n"
     ]
    }
   ],
   "source": [
    "X_train, X_train_text, Y_train, Y_train2 = split_into_XY(data_chng_train, train_text_vectors, 1, 30, 1)\n",
    "X_test, X_test_text, Y_test, Y_test2 = split_into_XY(data_chng_test, test_text_vectors, 1, 30, 1)\n",
    "\n",
    "\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 5))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_input = Input(shape=(30, 5), name='ts_input')\n",
    "text_input = Input(shape=(30, 100), name='text_input')\n",
    "\n",
    "lstm1 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(main_input)\n",
    "lstm1 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(lstm1)\n",
    "lstm1 = Flatten()(lstm1)\n",
    "\n",
    "lstm2 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(text_input)\n",
    "lstm2 = LSTM(10, return_sequences=True, recurrent_dropout=0.25, dropout=0.25, bias_initializer='ones')(lstm2)\n",
    "lstm2 = Flatten()(lstm2)\n",
    "\n",
    "lstms = concatenate([lstm1, lstm2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = Dense(64)(lstms)\n",
    "x1 = LeakyReLU()(x1)\n",
    "x1 = Dense(1, activation = 'linear', name='regression')(x1)\n",
    "\n",
    "x2 = Dense(64)(lstms)\n",
    "x2 = LeakyReLU()(x2)\n",
    "x2 = Dropout(0.9)(x2)\n",
    "x2 = Dense(1, activation = 'sigmoid', name = 'class')(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_model = Model(inputs=[main_input, text_input], \n",
    "              outputs=[x1, x2])\n",
    "opt = Nadam(lr=0.002, clipnorm = 0.5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=50, min_lr=0.000001, verbose=1)\n",
    "checkpointer = ModelCheckpoint(monitor='val_loss', filepath=\"model.hdf5\", verbose=1, save_best_only=True)\n",
    "final_model.compile(optimizer=opt, loss={'regression': 'mse', 'class': 'binary_crossentropy'}, loss_weights=[1., 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x1197dedd8> (None, 30, 5)\n",
      "<keras.engine.topology.InputLayer object at 0x117e4fa58> (None, 30, 100)\n",
      "<keras.layers.recurrent.LSTM object at 0x118422e80> (None, 30, 10)\n",
      "<keras.layers.recurrent.LSTM object at 0x118394048> (None, 30, 10)\n",
      "<keras.layers.recurrent.LSTM object at 0x117e4f8d0> (None, 30, 10)\n",
      "<keras.layers.recurrent.LSTM object at 0x1183943c8> (None, 30, 10)\n",
      "<keras.layers.core.Flatten object at 0x1197decc0> (None, 300)\n",
      "<keras.layers.core.Flatten object at 0x1193df748> (None, 300)\n",
      "<keras.layers.merge.Concatenate object at 0x10f3caef0> (None, 600)\n",
      "<keras.layers.core.Dense object at 0x11be016d8> (None, 64)\n",
      "<keras.layers.core.Dense object at 0x11be01358> (None, 64)\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x1197de9b0> (None, 64)\n",
      "<keras.layers.advanced_activations.LeakyReLU object at 0x11be012b0> (None, 64)\n",
      "<keras.layers.core.Dropout object at 0x119303d30> (None, 64)\n",
      "<keras.layers.core.Dense object at 0x1192e4748> (None, 1)\n",
      "<keras.layers.core.Dense object at 0x119303cc0> (None, 1)\n"
     ]
    }
   ],
   "source": [
    "for layer in final_model.layers:\n",
    "    print(layer, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAoACgAAD/4QCMRXhpZgAATU0AKgAAAAgABQESAAMAAAABAAEAAAEaAAUA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\nKACiiigD/9k=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('network.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1580 samples, validate on 347 samples\n",
      "Epoch 1/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1759 - regression_loss: 0.0381 - class_loss: 0.6891Epoch 00000: val_loss improved from 0.14942 to 0.14934, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1751 - regression_loss: 0.0372 - class_loss: 0.6894 - val_loss: 0.1493 - val_regression_loss: 0.0104 - val_class_loss: 0.6945\n",
      "Epoch 2/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1766 - regression_loss: 0.0370 - class_loss: 0.6976Epoch 00001: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1757 - regression_loss: 0.0362 - class_loss: 0.6974 - val_loss: 0.1495 - val_regression_loss: 0.0105 - val_class_loss: 0.6950\n",
      "Epoch 3/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1708 - regression_loss: 0.0322 - class_loss: 0.6932Epoch 00002: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1751 - regression_loss: 0.0363 - class_loss: 0.6940 - val_loss: 0.1594 - val_regression_loss: 0.0204 - val_class_loss: 0.6949\n",
      "Epoch 4/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1676 - regression_loss: 0.0288 - class_loss: 0.6944Epoch 00003: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1771 - regression_loss: 0.0384 - class_loss: 0.6936 - val_loss: 0.1527 - val_regression_loss: 0.0136 - val_class_loss: 0.6955\n",
      "Epoch 5/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1744 - regression_loss: 0.0362 - class_loss: 0.6906Epoch 00004: val_loss improved from 0.14934 to 0.14908, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1742 - regression_loss: 0.0360 - class_loss: 0.6909 - val_loss: 0.1491 - val_regression_loss: 0.0101 - val_class_loss: 0.6947\n",
      "Epoch 6/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1737 - regression_loss: 0.0351 - class_loss: 0.6929Epoch 00005: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1734 - regression_loss: 0.0349 - class_loss: 0.6925 - val_loss: 0.1500 - val_regression_loss: 0.0110 - val_class_loss: 0.6949\n",
      "Epoch 7/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1735 - regression_loss: 0.0353 - class_loss: 0.6909Epoch 00006: val_loss improved from 0.14908 to 0.14882, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1731 - regression_loss: 0.0348 - class_loss: 0.6917 - val_loss: 0.1488 - val_regression_loss: 0.0099 - val_class_loss: 0.6947\n",
      "Epoch 8/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1716 - regression_loss: 0.0329 - class_loss: 0.6933Epoch 00007: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1737 - regression_loss: 0.0351 - class_loss: 0.6931 - val_loss: 0.1504 - val_regression_loss: 0.0113 - val_class_loss: 0.6952\n",
      "Epoch 9/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1725 - regression_loss: 0.0343 - class_loss: 0.6910Epoch 00008: val_loss improved from 0.14882 to 0.14830, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1723 - regression_loss: 0.0339 - class_loss: 0.6917 - val_loss: 0.1483 - val_regression_loss: 0.0092 - val_class_loss: 0.6955\n",
      "Epoch 10/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1721 - regression_loss: 0.0334 - class_loss: 0.6932Epoch 00009: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1712 - regression_loss: 0.0327 - class_loss: 0.6927 - val_loss: 0.1485 - val_regression_loss: 0.0094 - val_class_loss: 0.6958\n",
      "Epoch 11/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1723 - regression_loss: 0.0330 - class_loss: 0.6962Epoch 00010: val_loss improved from 0.14830 to 0.14713, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1715 - regression_loss: 0.0323 - class_loss: 0.6960 - val_loss: 0.1471 - val_regression_loss: 0.0080 - val_class_loss: 0.6955\n",
      "Epoch 12/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1736 - regression_loss: 0.0354 - class_loss: 0.6907Epoch 00011: val_loss improved from 0.14713 to 0.14677, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1730 - regression_loss: 0.0347 - class_loss: 0.6913 - val_loss: 0.1468 - val_regression_loss: 0.0078 - val_class_loss: 0.6949\n",
      "Epoch 13/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1682 - regression_loss: 0.0296 - class_loss: 0.6931Epoch 00012: val_loss improved from 0.14677 to 0.14638, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1675 - regression_loss: 0.0290 - class_loss: 0.6926 - val_loss: 0.1464 - val_regression_loss: 0.0074 - val_class_loss: 0.6950\n",
      "Epoch 14/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1686 - regression_loss: 0.0300 - class_loss: 0.6928Epoch 00013: val_loss improved from 0.14638 to 0.14592, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1678 - regression_loss: 0.0293 - class_loss: 0.6925 - val_loss: 0.1459 - val_regression_loss: 0.0069 - val_class_loss: 0.6951\n",
      "Epoch 15/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1708 - regression_loss: 0.0320 - class_loss: 0.6941Epoch 00014: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1701 - regression_loss: 0.0312 - class_loss: 0.6945 - val_loss: 0.1459 - val_regression_loss: 0.0070 - val_class_loss: 0.6949\n",
      "Epoch 16/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1670 - regression_loss: 0.0289 - class_loss: 0.6906Epoch 00015: val_loss improved from 0.14592 to 0.14528, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1663 - regression_loss: 0.0282 - class_loss: 0.6903 - val_loss: 0.1453 - val_regression_loss: 0.0063 - val_class_loss: 0.6951\n",
      "Epoch 17/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1659 - regression_loss: 0.0279 - class_loss: 0.6902Epoch 00016: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1653 - regression_loss: 0.0273 - class_loss: 0.6901 - val_loss: 0.1453 - val_regression_loss: 0.0062 - val_class_loss: 0.6956\n",
      "Epoch 18/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1548 - regression_loss: 0.0165 - class_loss: 0.6915Epoch 00017: val_loss improved from 0.14528 to 0.14508, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1640 - regression_loss: 0.0257 - class_loss: 0.6914 - val_loss: 0.1451 - val_regression_loss: 0.0058 - val_class_loss: 0.6965\n",
      "Epoch 19/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1768 - regression_loss: 0.0383 - class_loss: 0.6924Epoch 00018: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1762 - regression_loss: 0.0376 - class_loss: 0.6931 - val_loss: 0.1459 - val_regression_loss: 0.0068 - val_class_loss: 0.6955\n",
      "Epoch 20/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1686 - regression_loss: 0.0304 - class_loss: 0.6910Epoch 00019: val_loss improved from 0.14508 to 0.14484, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1681 - regression_loss: 0.0299 - class_loss: 0.6913 - val_loss: 0.1448 - val_regression_loss: 0.0058 - val_class_loss: 0.6950\n",
      "Epoch 21/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1614 - regression_loss: 0.0230 - class_loss: 0.6923Epoch 00020: val_loss improved from 0.14484 to 0.14400, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1610 - regression_loss: 0.0225 - class_loss: 0.6925 - val_loss: 0.1440 - val_regression_loss: 0.0051 - val_class_loss: 0.6944\n",
      "Epoch 22/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1597 - regression_loss: 0.0208 - class_loss: 0.6946Epoch 00021: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1592 - regression_loss: 0.0203 - class_loss: 0.6942 - val_loss: 0.1448 - val_regression_loss: 0.0057 - val_class_loss: 0.6952\n",
      "Epoch 23/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1666 - regression_loss: 0.0281 - class_loss: 0.6924Epoch 00022: val_loss improved from 0.14400 to 0.14369, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1660 - regression_loss: 0.0276 - class_loss: 0.6922 - val_loss: 0.1437 - val_regression_loss: 0.0046 - val_class_loss: 0.6956\n",
      "Epoch 24/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1609 - regression_loss: 0.0228 - class_loss: 0.6904Epoch 00023: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1605 - regression_loss: 0.0225 - class_loss: 0.6902 - val_loss: 0.1438 - val_regression_loss: 0.0046 - val_class_loss: 0.6958\n",
      "Epoch 25/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1657 - regression_loss: 0.0268 - class_loss: 0.6945Epoch 00024: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1650 - regression_loss: 0.0262 - class_loss: 0.6940 - val_loss: 0.1448 - val_regression_loss: 0.0055 - val_class_loss: 0.6963\n",
      "Epoch 26/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1596 - regression_loss: 0.0214 - class_loss: 0.6912Epoch 00025: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1591 - regression_loss: 0.0209 - class_loss: 0.6908 - val_loss: 0.1439 - val_regression_loss: 0.0045 - val_class_loss: 0.6972\n",
      "Epoch 27/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1582 - regression_loss: 0.0199 - class_loss: 0.6918Epoch 00026: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1580 - regression_loss: 0.0195 - class_loss: 0.6925 - val_loss: 0.1447 - val_regression_loss: 0.0055 - val_class_loss: 0.6962\n",
      "Epoch 28/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1553 - regression_loss: 0.0172 - class_loss: 0.6908Epoch 00027: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1552 - regression_loss: 0.0170 - class_loss: 0.6907 - val_loss: 0.1438 - val_regression_loss: 0.0045 - val_class_loss: 0.6960\n",
      "Epoch 29/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1523 - regression_loss: 0.0142 - class_loss: 0.6902Epoch 00028: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1543 - regression_loss: 0.0161 - class_loss: 0.6911 - val_loss: 0.1512 - val_regression_loss: 0.0122 - val_class_loss: 0.6950\n",
      "Epoch 30/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1535 - regression_loss: 0.0153 - class_loss: 0.6910Epoch 00029: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1533 - regression_loss: 0.0151 - class_loss: 0.6909 - val_loss: 0.1447 - val_regression_loss: 0.0057 - val_class_loss: 0.6948\n",
      "Epoch 31/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1580 - regression_loss: 0.0195 - class_loss: 0.6924Epoch 00030: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1576 - regression_loss: 0.0191 - class_loss: 0.6923 - val_loss: 0.1443 - val_regression_loss: 0.0054 - val_class_loss: 0.6947\n",
      "Epoch 32/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1530 - regression_loss: 0.0149 - class_loss: 0.6906Epoch 00031: val_loss improved from 0.14369 to 0.14315, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1528 - regression_loss: 0.0147 - class_loss: 0.6904 - val_loss: 0.1432 - val_regression_loss: 0.0042 - val_class_loss: 0.6948\n",
      "Epoch 33/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1566 - regression_loss: 0.0183 - class_loss: 0.6917Epoch 00032: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1564 - regression_loss: 0.0179 - class_loss: 0.6924 - val_loss: 0.1433 - val_regression_loss: 0.0044 - val_class_loss: 0.6946\n",
      "Epoch 34/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1580 - regression_loss: 0.0200 - class_loss: 0.6901Epoch 00033: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1574 - regression_loss: 0.0195 - class_loss: 0.6894 - val_loss: 0.1443 - val_regression_loss: 0.0054 - val_class_loss: 0.6947\n",
      "Epoch 35/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1493 - regression_loss: 0.0111 - class_loss: 0.6910Epoch 00034: val_loss improved from 0.14315 to 0.14288, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1492 - regression_loss: 0.0110 - class_loss: 0.6911 - val_loss: 0.1429 - val_regression_loss: 0.0039 - val_class_loss: 0.6949\n",
      "Epoch 36/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1608 - regression_loss: 0.0228 - class_loss: 0.6901Epoch 00035: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1604 - regression_loss: 0.0223 - class_loss: 0.6902 - val_loss: 0.1439 - val_regression_loss: 0.0049 - val_class_loss: 0.6949\n",
      "Epoch 37/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1493 - regression_loss: 0.0108 - class_loss: 0.6925Epoch 00036: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1517 - regression_loss: 0.0133 - class_loss: 0.6922 - val_loss: 0.1604 - val_regression_loss: 0.0215 - val_class_loss: 0.6945\n",
      "Epoch 38/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1578 - regression_loss: 0.0195 - class_loss: 0.6915Epoch 00037: val_loss improved from 0.14288 to 0.14243, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1574 - regression_loss: 0.0191 - class_loss: 0.6913 - val_loss: 0.1424 - val_regression_loss: 0.0034 - val_class_loss: 0.6950\n",
      "Epoch 39/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1475 - regression_loss: 0.0093 - class_loss: 0.6909Epoch 00038: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1475 - regression_loss: 0.0092 - class_loss: 0.6914 - val_loss: 0.1428 - val_regression_loss: 0.0038 - val_class_loss: 0.6949\n",
      "Epoch 40/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1595 - regression_loss: 0.0212 - class_loss: 0.6915Epoch 00039: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1590 - regression_loss: 0.0208 - class_loss: 0.6912 - val_loss: 0.1428 - val_regression_loss: 0.0037 - val_class_loss: 0.6952\n",
      "Epoch 41/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1514 - regression_loss: 0.0134 - class_loss: 0.6901Epoch 00040: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1512 - regression_loss: 0.0132 - class_loss: 0.6902 - val_loss: 0.1429 - val_regression_loss: 0.0038 - val_class_loss: 0.6955\n",
      "Epoch 42/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1502 - regression_loss: 0.0121 - class_loss: 0.6906Epoch 00041: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1500 - regression_loss: 0.0119 - class_loss: 0.6905 - val_loss: 0.1428 - val_regression_loss: 0.0038 - val_class_loss: 0.6953\n",
      "Epoch 43/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1498 - regression_loss: 0.0116 - class_loss: 0.6911Epoch 00042: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1504 - regression_loss: 0.0122 - class_loss: 0.6912 - val_loss: 0.1458 - val_regression_loss: 0.0068 - val_class_loss: 0.6951\n",
      "Epoch 44/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1479 - regression_loss: 0.0097 - class_loss: 0.6908Epoch 00043: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1477 - regression_loss: 0.0096 - class_loss: 0.6904 - val_loss: 0.1430 - val_regression_loss: 0.0039 - val_class_loss: 0.6954\n",
      "Epoch 45/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1457 - regression_loss: 0.0073 - class_loss: 0.6919Epoch 00044: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1457 - regression_loss: 0.0074 - class_loss: 0.6915 - val_loss: 0.1429 - val_regression_loss: 0.0038 - val_class_loss: 0.6955\n",
      "Epoch 46/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1516 - regression_loss: 0.0136 - class_loss: 0.6900Epoch 00045: val_loss did not improve\n",
      "1580/1580 [==============================] - 3s - loss: 0.1513 - regression_loss: 0.0133 - class_loss: 0.6900 - val_loss: 0.1427 - val_regression_loss: 0.0035 - val_class_loss: 0.6958\n",
      "Epoch 47/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1504 - regression_loss: 0.0125 - class_loss: 0.6896Epoch 00046: val_loss did not improve\n",
      "1580/1580 [==============================] - 3s - loss: 0.1502 - regression_loss: 0.0123 - class_loss: 0.6898 - val_loss: 0.1425 - val_regression_loss: 0.0033 - val_class_loss: 0.6957\n",
      "Epoch 48/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1461 - regression_loss: 0.0079 - class_loss: 0.6911Epoch 00047: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1465 - regression_loss: 0.0083 - class_loss: 0.6909 - val_loss: 0.1441 - val_regression_loss: 0.0049 - val_class_loss: 0.6957\n",
      "Epoch 49/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1561 - regression_loss: 0.0179 - class_loss: 0.6911Epoch 00048: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1557 - regression_loss: 0.0176 - class_loss: 0.6907 - val_loss: 0.1428 - val_regression_loss: 0.0037 - val_class_loss: 0.6958\n",
      "Epoch 50/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1463 - regression_loss: 0.0079 - class_loss: 0.6925Epoch 00049: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1547 - regression_loss: 0.0162 - class_loss: 0.6920 - val_loss: 0.1651 - val_regression_loss: 0.0258 - val_class_loss: 0.6961\n",
      "Epoch 51/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1683 - regression_loss: 0.0302 - class_loss: 0.6901Epoch 00050: val_loss improved from 0.14243 to 0.14229, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1676 - regression_loss: 0.0295 - class_loss: 0.6905 - val_loss: 0.1423 - val_regression_loss: 0.0032 - val_class_loss: 0.6955\n",
      "Epoch 52/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1519 - regression_loss: 0.0139 - class_loss: 0.6905Epoch 00051: val_loss improved from 0.14229 to 0.14209, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1518 - regression_loss: 0.0137 - class_loss: 0.6906 - val_loss: 0.1421 - val_regression_loss: 0.0030 - val_class_loss: 0.6954\n",
      "Epoch 53/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1497 - regression_loss: 0.0117 - class_loss: 0.6900Epoch 00052: val_loss improved from 0.14209 to 0.14180, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1497 - regression_loss: 0.0115 - class_loss: 0.6906 - val_loss: 0.1418 - val_regression_loss: 0.0028 - val_class_loss: 0.6949\n",
      "Epoch 54/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1462 - regression_loss: 0.0079 - class_loss: 0.6911Epoch 00053: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1461 - regression_loss: 0.0079 - class_loss: 0.6911 - val_loss: 0.1419 - val_regression_loss: 0.0029 - val_class_loss: 0.6948\n",
      "Epoch 55/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1560 - regression_loss: 0.0179 - class_loss: 0.6906Epoch 00054: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1555 - regression_loss: 0.0175 - class_loss: 0.6902 - val_loss: 0.1420 - val_regression_loss: 0.0029 - val_class_loss: 0.6953\n",
      "Epoch 56/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1451 - regression_loss: 0.0070 - class_loss: 0.6905Epoch 00055: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1449 - regression_loss: 0.0069 - class_loss: 0.6900 - val_loss: 0.1418 - val_regression_loss: 0.0027 - val_class_loss: 0.6957\n",
      "Epoch 57/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1455 - regression_loss: 0.0077 - class_loss: 0.6888Epoch 00056: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1454 - regression_loss: 0.0076 - class_loss: 0.6891 - val_loss: 0.1419 - val_regression_loss: 0.0027 - val_class_loss: 0.6960\n",
      "Epoch 58/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1488 - regression_loss: 0.0109 - class_loss: 0.6895Epoch 00057: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1487 - regression_loss: 0.0107 - class_loss: 0.6901 - val_loss: 0.1418 - val_regression_loss: 0.0027 - val_class_loss: 0.6957\n",
      "Epoch 59/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1449 - regression_loss: 0.0069 - class_loss: 0.6903Epoch 00058: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1449 - regression_loss: 0.0067 - class_loss: 0.6911 - val_loss: 0.1418 - val_regression_loss: 0.0027 - val_class_loss: 0.6955\n",
      "Epoch 60/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1435 - regression_loss: 0.0054 - class_loss: 0.6908Epoch 00059: val_loss improved from 0.14180 to 0.14159, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1434 - regression_loss: 0.0054 - class_loss: 0.6904 - val_loss: 0.1416 - val_regression_loss: 0.0025 - val_class_loss: 0.6956\n",
      "Epoch 61/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1471 - regression_loss: 0.0089 - class_loss: 0.6907Epoch 00060: val_loss improved from 0.14159 to 0.14159, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1469 - regression_loss: 0.0088 - class_loss: 0.6904 - val_loss: 0.1416 - val_regression_loss: 0.0024 - val_class_loss: 0.6958\n",
      "Epoch 62/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1489 - regression_loss: 0.0111 - class_loss: 0.6888Epoch 00061: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1490 - regression_loss: 0.0111 - class_loss: 0.6896 - val_loss: 0.1418 - val_regression_loss: 0.0028 - val_class_loss: 0.6953\n",
      "Epoch 63/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1457 - regression_loss: 0.0076 - class_loss: 0.6906Epoch 00062: val_loss improved from 0.14159 to 0.14158, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1456 - regression_loss: 0.0076 - class_loss: 0.6899 - val_loss: 0.1416 - val_regression_loss: 0.0025 - val_class_loss: 0.6956\n",
      "Epoch 64/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1477 - regression_loss: 0.0097 - class_loss: 0.6899Epoch 00063: val_loss improved from 0.14158 to 0.14152, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1475 - regression_loss: 0.0095 - class_loss: 0.6898 - val_loss: 0.1415 - val_regression_loss: 0.0024 - val_class_loss: 0.6957\n",
      "Epoch 65/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1452 - regression_loss: 0.0073 - class_loss: 0.6893Epoch 00064: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1452 - regression_loss: 0.0072 - class_loss: 0.6896 - val_loss: 0.1415 - val_regression_loss: 0.0024 - val_class_loss: 0.6957\n",
      "Epoch 66/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1454 - regression_loss: 0.0072 - class_loss: 0.6911Epoch 00065: val_loss improved from 0.14152 to 0.14151, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1452 - regression_loss: 0.0071 - class_loss: 0.6906 - val_loss: 0.1415 - val_regression_loss: 0.0023 - val_class_loss: 0.6960\n",
      "Epoch 67/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1456 - regression_loss: 0.0075 - class_loss: 0.6904Epoch 00066: val_loss improved from 0.14151 to 0.14145, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1455 - regression_loss: 0.0074 - class_loss: 0.6901 - val_loss: 0.1414 - val_regression_loss: 0.0022 - val_class_loss: 0.6961\n",
      "Epoch 68/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1431 - regression_loss: 0.0054 - class_loss: 0.6888Epoch 00067: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1432 - regression_loss: 0.0054 - class_loss: 0.6892 - val_loss: 0.1415 - val_regression_loss: 0.0023 - val_class_loss: 0.6959\n",
      "Epoch 69/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1429 - regression_loss: 0.0049 - class_loss: 0.6903Epoch 00068: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1429 - regression_loss: 0.0049 - class_loss: 0.6902 - val_loss: 0.1417 - val_regression_loss: 0.0025 - val_class_loss: 0.6958\n",
      "Epoch 70/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1439 - regression_loss: 0.0061 - class_loss: 0.6891Epoch 00069: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1440 - regression_loss: 0.0060 - class_loss: 0.6897 - val_loss: 0.1415 - val_regression_loss: 0.0024 - val_class_loss: 0.6953\n",
      "Epoch 71/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1457 - regression_loss: 0.0076 - class_loss: 0.6908Epoch 00070: val_loss improved from 0.14145 to 0.14117, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1457 - regression_loss: 0.0076 - class_loss: 0.6904 - val_loss: 0.1412 - val_regression_loss: 0.0021 - val_class_loss: 0.6955\n",
      "Epoch 72/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1431 - regression_loss: 0.0050 - class_loss: 0.6908Epoch 00071: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1431 - regression_loss: 0.0050 - class_loss: 0.6907 - val_loss: 0.1413 - val_regression_loss: 0.0021 - val_class_loss: 0.6960\n",
      "Epoch 73/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1512 - regression_loss: 0.0134 - class_loss: 0.6891Epoch 00072: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1510 - regression_loss: 0.0131 - class_loss: 0.6893 - val_loss: 0.1414 - val_regression_loss: 0.0021 - val_class_loss: 0.6963\n",
      "Epoch 74/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1487 - regression_loss: 0.0106 - class_loss: 0.6905Epoch 00073: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1484 - regression_loss: 0.0103 - class_loss: 0.6902 - val_loss: 0.1415 - val_regression_loss: 0.0023 - val_class_loss: 0.6964\n",
      "Epoch 75/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1440 - regression_loss: 0.0061 - class_loss: 0.6895Epoch 00074: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1440 - regression_loss: 0.0061 - class_loss: 0.6898 - val_loss: 0.1413 - val_regression_loss: 0.0020 - val_class_loss: 0.6963\n",
      "Epoch 76/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1445 - regression_loss: 0.0065 - class_loss: 0.6903Epoch 00075: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1444 - regression_loss: 0.0065 - class_loss: 0.6895 - val_loss: 0.1419 - val_regression_loss: 0.0026 - val_class_loss: 0.6965\n",
      "Epoch 77/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1451 - regression_loss: 0.0072 - class_loss: 0.6891Epoch 00076: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1450 - regression_loss: 0.0071 - class_loss: 0.6894 - val_loss: 0.1414 - val_regression_loss: 0.0021 - val_class_loss: 0.6963\n",
      "Epoch 78/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1434 - regression_loss: 0.0053 - class_loss: 0.6903Epoch 00077: val_loss improved from 0.14117 to 0.14111, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1433 - regression_loss: 0.0053 - class_loss: 0.6898 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6963\n",
      "Epoch 79/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1431 - regression_loss: 0.0050 - class_loss: 0.6905Epoch 00078: val_loss improved from 0.14111 to 0.14107, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1430 - regression_loss: 0.0050 - class_loss: 0.6904 - val_loss: 0.1411 - val_regression_loss: 0.0018 - val_class_loss: 0.6961\n",
      "Epoch 80/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1433 - regression_loss: 0.0054 - class_loss: 0.6895Epoch 00079: val_loss improved from 0.14107 to 0.14096, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1432 - regression_loss: 0.0054 - class_loss: 0.6893 - val_loss: 0.1410 - val_regression_loss: 0.0018 - val_class_loss: 0.6961\n",
      "Epoch 81/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1530 - regression_loss: 0.0152 - class_loss: 0.6891Epoch 00080: val_loss improved from 0.14096 to 0.14088, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1527 - regression_loss: 0.0149 - class_loss: 0.6892 - val_loss: 0.1409 - val_regression_loss: 0.0017 - val_class_loss: 0.6959\n",
      "Epoch 82/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1482 - regression_loss: 0.0101 - class_loss: 0.6907Epoch 00081: val_loss improved from 0.14088 to 0.14077, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1480 - regression_loss: 0.0099 - class_loss: 0.6906 - val_loss: 0.1408 - val_regression_loss: 0.0016 - val_class_loss: 0.6957\n",
      "Epoch 83/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1442 - regression_loss: 0.0064 - class_loss: 0.6890Epoch 00082: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1443 - regression_loss: 0.0066 - class_loss: 0.6888 - val_loss: 0.1423 - val_regression_loss: 0.0031 - val_class_loss: 0.6958\n",
      "Epoch 84/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1438 - regression_loss: 0.0059 - class_loss: 0.6895Epoch 00083: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1438 - regression_loss: 0.0059 - class_loss: 0.6896 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6959\n",
      "Epoch 85/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1451 - regression_loss: 0.0070 - class_loss: 0.6903Epoch 00084: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1450 - regression_loss: 0.0069 - class_loss: 0.6902 - val_loss: 0.1415 - val_regression_loss: 0.0023 - val_class_loss: 0.6960\n",
      "Epoch 86/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1470 - regression_loss: 0.0090 - class_loss: 0.6899Epoch 00085: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1467 - regression_loss: 0.0088 - class_loss: 0.6893 - val_loss: 0.1409 - val_regression_loss: 0.0016 - val_class_loss: 0.6965\n",
      "Epoch 87/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1519 - regression_loss: 0.0140 - class_loss: 0.6897Epoch 00086: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1518 - regression_loss: 0.0138 - class_loss: 0.6901 - val_loss: 0.1409 - val_regression_loss: 0.0017 - val_class_loss: 0.6962\n",
      "Epoch 88/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1438 - regression_loss: 0.0056 - class_loss: 0.6907Epoch 00087: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1437 - regression_loss: 0.0055 - class_loss: 0.6906 - val_loss: 0.1411 - val_regression_loss: 0.0018 - val_class_loss: 0.6962\n",
      "Epoch 89/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1462 - regression_loss: 0.0082 - class_loss: 0.6899Epoch 00088: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1461 - regression_loss: 0.0081 - class_loss: 0.6901 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6960\n",
      "Epoch 90/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1526 - regression_loss: 0.0146 - class_loss: 0.6901Epoch 00089: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1522 - regression_loss: 0.0143 - class_loss: 0.6896 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6961\n",
      "Epoch 91/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1464 - regression_loss: 0.0082 - class_loss: 0.6909Epoch 00090: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1462 - regression_loss: 0.0080 - class_loss: 0.6906 - val_loss: 0.1413 - val_regression_loss: 0.0020 - val_class_loss: 0.6965\n",
      "Epoch 92/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1446 - regression_loss: 0.0064 - class_loss: 0.6911Epoch 00091: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1445 - regression_loss: 0.0063 - class_loss: 0.6909 - val_loss: 0.1411 - val_regression_loss: 0.0018 - val_class_loss: 0.6964\n",
      "Epoch 93/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1442 - regression_loss: 0.0065 - class_loss: 0.6883Epoch 00092: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1440 - regression_loss: 0.0064 - class_loss: 0.6881 - val_loss: 0.1414 - val_regression_loss: 0.0021 - val_class_loss: 0.6966\n",
      "Epoch 94/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1422 - regression_loss: 0.0041 - class_loss: 0.6905Epoch 00093: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1422 - regression_loss: 0.0041 - class_loss: 0.6904 - val_loss: 0.1410 - val_regression_loss: 0.0017 - val_class_loss: 0.6965\n",
      "Epoch 95/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1428 - regression_loss: 0.0047 - class_loss: 0.6905Epoch 00094: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1428 - regression_loss: 0.0047 - class_loss: 0.6905 - val_loss: 0.1426 - val_regression_loss: 0.0033 - val_class_loss: 0.6966\n",
      "Epoch 96/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1426 - regression_loss: 0.0046 - class_loss: 0.6900Epoch 00095: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1426 - regression_loss: 0.0046 - class_loss: 0.6898 - val_loss: 0.1411 - val_regression_loss: 0.0018 - val_class_loss: 0.6966\n",
      "Epoch 97/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1461 - regression_loss: 0.0085 - class_loss: 0.6881Epoch 00096: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1460 - regression_loss: 0.0083 - class_loss: 0.6886 - val_loss: 0.1408 - val_regression_loss: 0.0015 - val_class_loss: 0.6964\n",
      "Epoch 98/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1499 - regression_loss: 0.0118 - class_loss: 0.6905Epoch 00097: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1496 - regression_loss: 0.0115 - class_loss: 0.6905 - val_loss: 0.1408 - val_regression_loss: 0.0016 - val_class_loss: 0.6964\n",
      "Epoch 99/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1434 - regression_loss: 0.0054 - class_loss: 0.6899Epoch 00098: val_loss improved from 0.14077 to 0.14071, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1434 - regression_loss: 0.0054 - class_loss: 0.6901 - val_loss: 0.1407 - val_regression_loss: 0.0015 - val_class_loss: 0.6962\n",
      "Epoch 100/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1456 - regression_loss: 0.0076 - class_loss: 0.6902Epoch 00099: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1455 - regression_loss: 0.0074 - class_loss: 0.6903 - val_loss: 0.1407 - val_regression_loss: 0.0015 - val_class_loss: 0.6959\n",
      "Epoch 101/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1459 - regression_loss: 0.0081 - class_loss: 0.6888Epoch 00100: val_loss improved from 0.14071 to 0.14059, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1458 - regression_loss: 0.0080 - class_loss: 0.6891 - val_loss: 0.1406 - val_regression_loss: 0.0014 - val_class_loss: 0.6958\n",
      "Epoch 102/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1439 - regression_loss: 0.0059 - class_loss: 0.6902Epoch 00101: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1438 - regression_loss: 0.0059 - class_loss: 0.6897 - val_loss: 0.1407 - val_regression_loss: 0.0016 - val_class_loss: 0.6957\n",
      "Epoch 103/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1427 - regression_loss: 0.0045 - class_loss: 0.6906Epoch 00102: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1426 - regression_loss: 0.0045 - class_loss: 0.6905 - val_loss: 0.1408 - val_regression_loss: 0.0016 - val_class_loss: 0.6958\n",
      "Epoch 104/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1438 - regression_loss: 0.0062 - class_loss: 0.6880Epoch 00103: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1446 - regression_loss: 0.0070 - class_loss: 0.6881 - val_loss: 0.1533 - val_regression_loss: 0.0141 - val_class_loss: 0.6959\n",
      "Epoch 105/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1490 - regression_loss: 0.0111 - class_loss: 0.6895Epoch 00104: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1489 - regression_loss: 0.0110 - class_loss: 0.6896 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6959\n",
      "Epoch 106/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0043 - class_loss: 0.6878Epoch 00105: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0044 - class_loss: 0.6880 - val_loss: 0.1413 - val_regression_loss: 0.0021 - val_class_loss: 0.6958\n",
      "Epoch 107/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1566 - regression_loss: 0.0187 - class_loss: 0.6897Epoch 00106: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1561 - regression_loss: 0.0182 - class_loss: 0.6894 - val_loss: 0.1410 - val_regression_loss: 0.0018 - val_class_loss: 0.6960\n",
      "Epoch 108/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0036 - class_loss: 0.6899Epoch 00107: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1416 - regression_loss: 0.0036 - class_loss: 0.6897 - val_loss: 0.1407 - val_regression_loss: 0.0015 - val_class_loss: 0.6960\n",
      "Epoch 109/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0043 - class_loss: 0.6885Epoch 00108: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0043 - class_loss: 0.6884 - val_loss: 0.1411 - val_regression_loss: 0.0018 - val_class_loss: 0.6962\n",
      "Epoch 110/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1426 - regression_loss: 0.0045 - class_loss: 0.6904Epoch 00109: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0044 - class_loss: 0.6898 - val_loss: 0.1409 - val_regression_loss: 0.0016 - val_class_loss: 0.6964\n",
      "Epoch 111/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1432 - regression_loss: 0.0055 - class_loss: 0.6886Epoch 00110: val_loss improved from 0.14059 to 0.14052, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1433 - regression_loss: 0.0055 - class_loss: 0.6893 - val_loss: 0.1405 - val_regression_loss: 0.0013 - val_class_loss: 0.6961\n",
      "Epoch 112/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1467 - regression_loss: 0.0087 - class_loss: 0.6903Epoch 00111: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1466 - regression_loss: 0.0085 - class_loss: 0.6903 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6961\n",
      "Epoch 113/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0046 - class_loss: 0.6896Epoch 00112: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0045 - class_loss: 0.6895 - val_loss: 0.1406 - val_regression_loss: 0.0014 - val_class_loss: 0.6958\n",
      "Epoch 114/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1473 - regression_loss: 0.0094 - class_loss: 0.6895Epoch 00113: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1534 - regression_loss: 0.0156 - class_loss: 0.6889 - val_loss: 0.1858 - val_regression_loss: 0.0466 - val_class_loss: 0.6958\n",
      "Epoch 115/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1571 - regression_loss: 0.0195 - class_loss: 0.6884Epoch 00114: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1567 - regression_loss: 0.0190 - class_loss: 0.6884 - val_loss: 0.1412 - val_regression_loss: 0.0019 - val_class_loss: 0.6963\n",
      "Epoch 116/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1441 - regression_loss: 0.0063 - class_loss: 0.6893Epoch 00115: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1441 - regression_loss: 0.0062 - class_loss: 0.6895 - val_loss: 0.1414 - val_regression_loss: 0.0021 - val_class_loss: 0.6964\n",
      "Epoch 117/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1464 - regression_loss: 0.0084 - class_loss: 0.6901Epoch 00116: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1462 - regression_loss: 0.0083 - class_loss: 0.6897 - val_loss: 0.1410 - val_regression_loss: 0.0017 - val_class_loss: 0.6962\n",
      "Epoch 118/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1432 - regression_loss: 0.0052 - class_loss: 0.6899Epoch 00117: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1433 - regression_loss: 0.0053 - class_loss: 0.6904 - val_loss: 0.1408 - val_regression_loss: 0.0017 - val_class_loss: 0.6958\n",
      "Epoch 119/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0036 - class_loss: 0.6899Epoch 00118: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0036 - class_loss: 0.6895 - val_loss: 0.1407 - val_regression_loss: 0.0016 - val_class_loss: 0.6957\n",
      "Epoch 120/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1446 - regression_loss: 0.0067 - class_loss: 0.6896Epoch 00119: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1449 - regression_loss: 0.0067 - class_loss: 0.6908 - val_loss: 0.1405 - val_regression_loss: 0.0014 - val_class_loss: 0.6955\n",
      "Epoch 121/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1426 - regression_loss: 0.0043 - class_loss: 0.6913Epoch 00120: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0043 - class_loss: 0.6909 - val_loss: 0.1411 - val_regression_loss: 0.0019 - val_class_loss: 0.6958\n",
      "Epoch 122/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1472 - regression_loss: 0.0093 - class_loss: 0.6895Epoch 00121: val_loss improved from 0.14052 to 0.14038, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1472 - regression_loss: 0.0092 - class_loss: 0.6901 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6956\n",
      "Epoch 123/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0038 - class_loss: 0.6906Epoch 00122: val_loss improved from 0.14038 to 0.14035, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1419 - regression_loss: 0.0038 - class_loss: 0.6905 - val_loss: 0.1403 - val_regression_loss: 0.0013 - val_class_loss: 0.6954\n",
      "Epoch 124/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0045 - class_loss: 0.6900Epoch 00123: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0045 - class_loss: 0.6892 - val_loss: 0.1407 - val_regression_loss: 0.0015 - val_class_loss: 0.6957\n",
      "Epoch 125/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1434 - regression_loss: 0.0053 - class_loss: 0.6902Epoch 00124: val_loss improved from 0.14035 to 0.14030, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1432 - regression_loss: 0.0052 - class_loss: 0.6898 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6958\n",
      "Epoch 126/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1427 - regression_loss: 0.0047 - class_loss: 0.6898Epoch 00125: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1427 - regression_loss: 0.0048 - class_loss: 0.6898 - val_loss: 0.1408 - val_regression_loss: 0.0016 - val_class_loss: 0.6960\n",
      "Epoch 127/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1441 - regression_loss: 0.0060 - class_loss: 0.6904Epoch 00126: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1439 - regression_loss: 0.0059 - class_loss: 0.6898 - val_loss: 0.1404 - val_regression_loss: 0.0011 - val_class_loss: 0.6964\n",
      "Epoch 128/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1442 - regression_loss: 0.0063 - class_loss: 0.6896Epoch 00127: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1441 - regression_loss: 0.0062 - class_loss: 0.6893 - val_loss: 0.1408 - val_regression_loss: 0.0015 - val_class_loss: 0.6966\n",
      "Epoch 129/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1421 - regression_loss: 0.0044 - class_loss: 0.6883Epoch 00128: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0044 - class_loss: 0.6887 - val_loss: 0.1408 - val_regression_loss: 0.0016 - val_class_loss: 0.6961\n",
      "Epoch 130/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1451 - regression_loss: 0.0073 - class_loss: 0.6892Epoch 00129: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1452 - regression_loss: 0.0073 - class_loss: 0.6896 - val_loss: 0.1420 - val_regression_loss: 0.0029 - val_class_loss: 0.6957\n",
      "Epoch 131/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1428 - regression_loss: 0.0050 - class_loss: 0.6887Epoch 00130: val_loss improved from 0.14030 to 0.14027, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1427 - regression_loss: 0.0049 - class_loss: 0.6886 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6956\n",
      "Epoch 132/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1458 - regression_loss: 0.0079 - class_loss: 0.6893Epoch 00131: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1456 - regression_loss: 0.0078 - class_loss: 0.6894 - val_loss: 0.1405 - val_regression_loss: 0.0014 - val_class_loss: 0.6953\n",
      "Epoch 133/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1429 - regression_loss: 0.0047 - class_loss: 0.6912Epoch 00132: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1428 - regression_loss: 0.0046 - class_loss: 0.6906 - val_loss: 0.1406 - val_regression_loss: 0.0015 - val_class_loss: 0.6955\n",
      "Epoch 134/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0032 - class_loss: 0.6900Epoch 00133: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0033 - class_loss: 0.6906 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6956\n",
      "Epoch 135/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0039 - class_loss: 0.6895Epoch 00134: val_loss improved from 0.14027 to 0.14025, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1418 - regression_loss: 0.0039 - class_loss: 0.6895 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6956\n",
      "Epoch 136/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0039 - class_loss: 0.6888Epoch 00135: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1417 - regression_loss: 0.0040 - class_loss: 0.6888 - val_loss: 0.1406 - val_regression_loss: 0.0015 - val_class_loss: 0.6956\n",
      "Epoch 137/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1436 - regression_loss: 0.0057 - class_loss: 0.6895Epoch 00136: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1435 - regression_loss: 0.0056 - class_loss: 0.6894 - val_loss: 0.1410 - val_regression_loss: 0.0019 - val_class_loss: 0.6955\n",
      "Epoch 138/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1498 - regression_loss: 0.0120 - class_loss: 0.6886Epoch 00137: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1495 - regression_loss: 0.0118 - class_loss: 0.6887 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6957\n",
      "Epoch 139/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1417 - regression_loss: 0.0036 - class_loss: 0.6902Epoch 00138: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1416 - regression_loss: 0.0036 - class_loss: 0.6903 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6957\n",
      "Epoch 140/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1429 - regression_loss: 0.0049 - class_loss: 0.6899Epoch 00139: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1430 - regression_loss: 0.0050 - class_loss: 0.6902 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6956\n",
      "Epoch 141/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1422 - regression_loss: 0.0044 - class_loss: 0.6890Epoch 00140: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1422 - regression_loss: 0.0043 - class_loss: 0.6893 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6955\n",
      "Epoch 142/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0043 - class_loss: 0.6879Epoch 00141: val_loss improved from 0.14025 to 0.14014, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0043 - class_loss: 0.6884 - val_loss: 0.1401 - val_regression_loss: 0.0011 - val_class_loss: 0.6953\n",
      "Epoch 143/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0043 - class_loss: 0.6883Epoch 00142: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0043 - class_loss: 0.6886 - val_loss: 0.1404 - val_regression_loss: 0.0014 - val_class_loss: 0.6953\n",
      "Epoch 144/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0035 - class_loss: 0.6903Epoch 00143: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0035 - class_loss: 0.6903 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6955\n",
      "Epoch 145/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1427 - regression_loss: 0.0048 - class_loss: 0.6894Epoch 00144: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1426 - regression_loss: 0.0047 - class_loss: 0.6894 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6957\n",
      "Epoch 146/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1441 - regression_loss: 0.0064 - class_loss: 0.6886Epoch 00145: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1442 - regression_loss: 0.0063 - class_loss: 0.6895 - val_loss: 0.1402 - val_regression_loss: 0.0012 - val_class_loss: 0.6953\n",
      "Epoch 147/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0036 - class_loss: 0.6894Epoch 00146: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0035 - class_loss: 0.6897 - val_loss: 0.1407 - val_regression_loss: 0.0017 - val_class_loss: 0.6952\n",
      "Epoch 148/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0047 - class_loss: 0.6889Epoch 00147: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0046 - class_loss: 0.6889 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6953\n",
      "Epoch 149/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1452 - regression_loss: 0.0069 - class_loss: 0.6914Epoch 00148: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1449 - regression_loss: 0.0068 - class_loss: 0.6903 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6958\n",
      "Epoch 150/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1413 - regression_loss: 0.0034 - class_loss: 0.6892Epoch 00149: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0036 - class_loss: 0.6890 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6963\n",
      "Epoch 151/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1452 - regression_loss: 0.0067 - class_loss: 0.6923Epoch 00150: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1450 - regression_loss: 0.0066 - class_loss: 0.6919 - val_loss: 0.1404 - val_regression_loss: 0.0011 - val_class_loss: 0.6964\n",
      "Epoch 152/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0034 - class_loss: 0.6901Epoch 00151: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0034 - class_loss: 0.6899 - val_loss: 0.1403 - val_regression_loss: 0.0010 - val_class_loss: 0.6963\n",
      "Epoch 153/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1490 - regression_loss: 0.0112 - class_loss: 0.6891Epoch 00152: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1491 - regression_loss: 0.0112 - class_loss: 0.6897 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6960\n",
      "Epoch 154/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0040 - class_loss: 0.6893Epoch 00153: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1419 - regression_loss: 0.0040 - class_loss: 0.6893 - val_loss: 0.1405 - val_regression_loss: 0.0013 - val_class_loss: 0.6961\n",
      "Epoch 155/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0041 - class_loss: 0.6893Epoch 00154: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1419 - regression_loss: 0.0040 - class_loss: 0.6894 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6959\n",
      "Epoch 156/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1484 - regression_loss: 0.0107 - class_loss: 0.6885Epoch 00155: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1481 - regression_loss: 0.0105 - class_loss: 0.6882 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6960\n",
      "Epoch 157/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0038 - class_loss: 0.6901Epoch 00156: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1418 - regression_loss: 0.0037 - class_loss: 0.6901 - val_loss: 0.1402 - val_regression_loss: 0.0010 - val_class_loss: 0.6957\n",
      "Epoch 158/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1424 - regression_loss: 0.0046 - class_loss: 0.6890Epoch 00157: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0045 - class_loss: 0.6888 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6958\n",
      "Epoch 159/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1527 - regression_loss: 0.0148 - class_loss: 0.6890Epoch 00158: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1523 - regression_loss: 0.0145 - class_loss: 0.6889 - val_loss: 0.1402 - val_regression_loss: 9.8151e-04 - val_class_loss: 0.6959\n",
      "Epoch 160/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0040 - class_loss: 0.6895Epoch 00159: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0041 - class_loss: 0.6898 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6959\n",
      "Epoch 161/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1423 - regression_loss: 0.0042 - class_loss: 0.6908Epoch 00160: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0041 - class_loss: 0.6907 - val_loss: 0.1407 - val_regression_loss: 0.0014 - val_class_loss: 0.6962\n",
      "Epoch 162/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1423 - regression_loss: 0.0044 - class_loss: 0.6892Epoch 00161: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1422 - regression_loss: 0.0044 - class_loss: 0.6892 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6961\n",
      "Epoch 163/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1430 - regression_loss: 0.0050 - class_loss: 0.6900Epoch 00162: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1429 - regression_loss: 0.0051 - class_loss: 0.6892 - val_loss: 0.1404 - val_regression_loss: 0.0011 - val_class_loss: 0.6964\n",
      "Epoch 164/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1486 - regression_loss: 0.0105 - class_loss: 0.6903Epoch 00163: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1484 - regression_loss: 0.0103 - class_loss: 0.6903 - val_loss: 0.1405 - val_regression_loss: 0.0011 - val_class_loss: 0.6967\n",
      "Epoch 165/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0043 - class_loss: 0.6880Epoch 00164: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1419 - regression_loss: 0.0042 - class_loss: 0.6885 - val_loss: 0.1406 - val_regression_loss: 0.0013 - val_class_loss: 0.6964\n",
      "Epoch 166/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0035 - class_loss: 0.6903Epoch 00165: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0034 - class_loss: 0.6899 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6963\n",
      "Epoch 167/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1436 - regression_loss: 0.0057 - class_loss: 0.6895Epoch 00166: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1436 - regression_loss: 0.0057 - class_loss: 0.6897 - val_loss: 0.1402 - val_regression_loss: 0.0010 - val_class_loss: 0.6959\n",
      "Epoch 168/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1489 - regression_loss: 0.0110 - class_loss: 0.6892Epoch 00167: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1486 - regression_loss: 0.0108 - class_loss: 0.6891 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6959\n",
      "Epoch 169/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1513 - regression_loss: 0.0135 - class_loss: 0.6890Epoch 00168: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1510 - regression_loss: 0.0132 - class_loss: 0.6891 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6957\n",
      "Epoch 170/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1440 - regression_loss: 0.0058 - class_loss: 0.6910Epoch 00169: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1438 - regression_loss: 0.0057 - class_loss: 0.6908 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6959\n",
      "Epoch 171/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1433 - regression_loss: 0.0054 - class_loss: 0.6895Epoch 00170: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1434 - regression_loss: 0.0054 - class_loss: 0.6901 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6957\n",
      "Epoch 172/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1408 - regression_loss: 0.0029 - class_loss: 0.6895Epoch 00171: val_loss improved from 0.14014 to 0.14006, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0029 - class_loss: 0.6894 - val_loss: 0.1401 - val_regression_loss: 9.9721e-04 - val_class_loss: 0.6953\n",
      "Epoch 173/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1436 - regression_loss: 0.0057 - class_loss: 0.6893Epoch 00172: val_loss improved from 0.14006 to 0.14000, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1435 - regression_loss: 0.0056 - class_loss: 0.6896 - val_loss: 0.1400 - val_regression_loss: 9.8570e-04 - val_class_loss: 0.6951\n",
      "Epoch 174/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0027 - class_loss: 0.6898Epoch 00173: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1416 - regression_loss: 0.0036 - class_loss: 0.6899 - val_loss: 0.1549 - val_regression_loss: 0.0159 - val_class_loss: 0.6950\n",
      "Epoch 175/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1487 - regression_loss: 0.0108 - class_loss: 0.6894Epoch 00174: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1485 - regression_loss: 0.0106 - class_loss: 0.6893 - val_loss: 0.1406 - val_regression_loss: 0.0015 - val_class_loss: 0.6954\n",
      "Epoch 176/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1415 - regression_loss: 0.0038 - class_loss: 0.6885Epoch 00175: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0037 - class_loss: 0.6887 - val_loss: 0.1406 - val_regression_loss: 0.0015 - val_class_loss: 0.6954\n",
      "Epoch 177/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1421 - regression_loss: 0.0045 - class_loss: 0.6882Epoch 00176: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0045 - class_loss: 0.6890 - val_loss: 0.1406 - val_regression_loss: 0.0015 - val_class_loss: 0.6952\n",
      "Epoch 178/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0042 - class_loss: 0.6882Epoch 00177: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1418 - regression_loss: 0.0041 - class_loss: 0.6885 - val_loss: 0.1403 - val_regression_loss: 0.0013 - val_class_loss: 0.6949\n",
      "Epoch 179/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1430 - regression_loss: 0.0051 - class_loss: 0.6891Epoch 00178: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1429 - regression_loss: 0.0050 - class_loss: 0.6895 - val_loss: 0.1404 - val_regression_loss: 0.0014 - val_class_loss: 0.6950\n",
      "Epoch 180/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1441 - regression_loss: 0.0061 - class_loss: 0.6898Epoch 00179: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1439 - regression_loss: 0.0060 - class_loss: 0.6895 - val_loss: 0.1403 - val_regression_loss: 0.0013 - val_class_loss: 0.6952\n",
      "Epoch 181/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1447 - regression_loss: 0.0070 - class_loss: 0.6887Epoch 00180: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1446 - regression_loss: 0.0068 - class_loss: 0.6886 - val_loss: 0.1402 - val_regression_loss: 0.0012 - val_class_loss: 0.6952\n",
      "Epoch 182/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0041 - class_loss: 0.6893Epoch 00181: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0041 - class_loss: 0.6901 - val_loss: 0.1403 - val_regression_loss: 0.0013 - val_class_loss: 0.6951\n",
      "Epoch 183/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1456 - regression_loss: 0.0080 - class_loss: 0.6878Epoch 00182: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1454 - regression_loss: 0.0079 - class_loss: 0.6877 - val_loss: 0.1402 - val_regression_loss: 0.0013 - val_class_loss: 0.6948\n",
      "Epoch 184/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1437 - regression_loss: 0.0058 - class_loss: 0.6895Epoch 00183: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1436 - regression_loss: 0.0057 - class_loss: 0.6898 - val_loss: 0.1402 - val_regression_loss: 0.0012 - val_class_loss: 0.6948\n",
      "Epoch 185/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0042 - class_loss: 0.6885Epoch 00184: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0044 - class_loss: 0.6886 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6953\n",
      "Epoch 186/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0040 - class_loss: 0.6893Epoch 00185: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1419 - regression_loss: 0.0040 - class_loss: 0.6895 - val_loss: 0.1407 - val_regression_loss: 0.0016 - val_class_loss: 0.6954\n",
      "Epoch 187/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1498 - regression_loss: 0.0118 - class_loss: 0.6901Epoch 00186: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1494 - regression_loss: 0.0115 - class_loss: 0.6896 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6957\n",
      "Epoch 188/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0027 - class_loss: 0.6895Epoch 00187: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1407 - regression_loss: 0.0029 - class_loss: 0.6894 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6954\n",
      "Epoch 189/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1428 - regression_loss: 0.0051 - class_loss: 0.6886Epoch 00188: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1428 - regression_loss: 0.0051 - class_loss: 0.6883 - val_loss: 0.1401 - val_regression_loss: 9.8445e-04 - val_class_loss: 0.6955\n",
      "Epoch 190/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1513 - regression_loss: 0.0134 - class_loss: 0.6896Epoch 00189: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1509 - regression_loss: 0.0131 - class_loss: 0.6894 - val_loss: 0.1401 - val_regression_loss: 9.3933e-04 - val_class_loss: 0.6957\n",
      "Epoch 191/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0027 - class_loss: 0.6891Epoch 00190: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0028 - class_loss: 0.6893 - val_loss: 0.1401 - val_regression_loss: 0.0010 - val_class_loss: 0.6953\n",
      "Epoch 192/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1421 - regression_loss: 0.0043 - class_loss: 0.6888Epoch 00191: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0046 - class_loss: 0.6890 - val_loss: 0.1425 - val_regression_loss: 0.0035 - val_class_loss: 0.6953\n",
      "Epoch 193/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1430 - regression_loss: 0.0055 - class_loss: 0.6876Epoch 00192: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1430 - regression_loss: 0.0054 - class_loss: 0.6878 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6953\n",
      "Epoch 194/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0033 - class_loss: 0.6895Epoch 00193: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0034 - class_loss: 0.6906 - val_loss: 0.1403 - val_regression_loss: 0.0013 - val_class_loss: 0.6951\n",
      "Epoch 195/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0025 - class_loss: 0.6895Epoch 00194: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0025 - class_loss: 0.6889 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6958\n",
      "Epoch 196/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0030 - class_loss: 0.6898Epoch 00195: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6892 - val_loss: 0.1405 - val_regression_loss: 0.0012 - val_class_loss: 0.6963\n",
      "Epoch 197/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0030 - class_loss: 0.6910Epoch 00196: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1411 - regression_loss: 0.0030 - class_loss: 0.6904 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6956\n",
      "Epoch 198/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0028 - class_loss: 0.6884Epoch 00197: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0028 - class_loss: 0.6889 - val_loss: 0.1402 - val_regression_loss: 0.0012 - val_class_loss: 0.6954\n",
      "Epoch 199/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1424 - regression_loss: 0.0045 - class_loss: 0.6891Epoch 00198: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0045 - class_loss: 0.6892 - val_loss: 0.1401 - val_regression_loss: 0.0010 - val_class_loss: 0.6953\n",
      "Epoch 200/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0038 - class_loss: 0.6897Epoch 00199: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1418 - regression_loss: 0.0038 - class_loss: 0.6900 - val_loss: 0.1401 - val_regression_loss: 0.0011 - val_class_loss: 0.6951\n",
      "Epoch 201/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0031 - class_loss: 0.6889Epoch 00200: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0031 - class_loss: 0.6887 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6953\n",
      "Epoch 202/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1446 - regression_loss: 0.0073 - class_loss: 0.6868Epoch 00201: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1447 - regression_loss: 0.0073 - class_loss: 0.6870 - val_loss: 0.1411 - val_regression_loss: 0.0020 - val_class_loss: 0.6953\n",
      "Epoch 203/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6886Epoch 00202: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6888 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6955\n",
      "Epoch 204/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0039 - class_loss: 0.6896Epoch 00203: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1417 - regression_loss: 0.0039 - class_loss: 0.6893 - val_loss: 0.1402 - val_regression_loss: 0.0010 - val_class_loss: 0.6959\n",
      "Epoch 205/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1413 - regression_loss: 0.0036 - class_loss: 0.6887Epoch 00204: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0036 - class_loss: 0.6881 - val_loss: 0.1405 - val_regression_loss: 0.0012 - val_class_loss: 0.6966\n",
      "Epoch 206/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1429 - regression_loss: 0.0050 - class_loss: 0.6891Epoch 00205: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1428 - regression_loss: 0.0050 - class_loss: 0.6891 - val_loss: 0.1403 - val_regression_loss: 9.8596e-04 - val_class_loss: 0.6965\n",
      "Epoch 207/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0025 - class_loss: 0.6900Epoch 00206: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1429 - regression_loss: 0.0051 - class_loss: 0.6892 - val_loss: 0.1516 - val_regression_loss: 0.0123 - val_class_loss: 0.6965\n",
      "Epoch 208/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1488 - regression_loss: 0.0105 - class_loss: 0.6915Epoch 00207: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1485 - regression_loss: 0.0103 - class_loss: 0.6912 - val_loss: 0.1406 - val_regression_loss: 0.0013 - val_class_loss: 0.6966\n",
      "Epoch 209/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1410 - regression_loss: 0.0032 - class_loss: 0.6889Epoch 00208: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0032 - class_loss: 0.6901 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6956\n",
      "Epoch 210/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1462 - regression_loss: 0.0086 - class_loss: 0.6883Epoch 00209: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1460 - regression_loss: 0.0085 - class_loss: 0.6877 - val_loss: 0.1404 - val_regression_loss: 0.0013 - val_class_loss: 0.6956\n",
      "Epoch 211/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1417 - regression_loss: 0.0041 - class_loss: 0.6883Epoch 00210: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1417 - regression_loss: 0.0041 - class_loss: 0.6883 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6959\n",
      "Epoch 212/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1426 - regression_loss: 0.0048 - class_loss: 0.6887Epoch 00211: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1425 - regression_loss: 0.0047 - class_loss: 0.6887 - val_loss: 0.1403 - val_regression_loss: 0.0010 - val_class_loss: 0.6960\n",
      "Epoch 213/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1415 - regression_loss: 0.0034 - class_loss: 0.6908Epoch 00212: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0033 - class_loss: 0.6906 - val_loss: 0.1402 - val_regression_loss: 9.9641e-04 - val_class_loss: 0.6960\n",
      "Epoch 214/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0026 - class_loss: 0.6891Epoch 00213: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0026 - class_loss: 0.6893 - val_loss: 0.1402 - val_regression_loss: 9.9387e-04 - val_class_loss: 0.6962\n",
      "Epoch 215/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0036 - class_loss: 0.6889Epoch 00214: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0035 - class_loss: 0.6892 - val_loss: 0.1402 - val_regression_loss: 9.4072e-04 - val_class_loss: 0.6962\n",
      "Epoch 216/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0028 - class_loss: 0.6869Epoch 00215: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0028 - class_loss: 0.6872 - val_loss: 0.1401 - val_regression_loss: 9.6120e-04 - val_class_loss: 0.6959\n",
      "Epoch 217/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1421 - regression_loss: 0.0042 - class_loss: 0.6893Epoch 00216: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0042 - class_loss: 0.6894 - val_loss: 0.1402 - val_regression_loss: 9.6948e-04 - val_class_loss: 0.6962\n",
      "Epoch 218/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0048 - class_loss: 0.6885Epoch 00217: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0047 - class_loss: 0.6883 - val_loss: 0.1402 - val_regression_loss: 9.8452e-04 - val_class_loss: 0.6963\n",
      "Epoch 219/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0026 - class_loss: 0.6879Epoch 00218: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0026 - class_loss: 0.6880 - val_loss: 0.1405 - val_regression_loss: 0.0014 - val_class_loss: 0.6955\n",
      "Epoch 220/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6887Epoch 00219: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0031 - class_loss: 0.6886 - val_loss: 0.1403 - val_regression_loss: 0.0013 - val_class_loss: 0.6948\n",
      "Epoch 221/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0030 - class_loss: 0.6908Epoch 00220: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0031 - class_loss: 0.6905 - val_loss: 0.1406 - val_regression_loss: 0.0015 - val_class_loss: 0.6956\n",
      "Epoch 222/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0028 - class_loss: 0.6879Epoch 00221: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0028 - class_loss: 0.6879 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6960\n",
      "Epoch 223/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0038 - class_loss: 0.6880Epoch 00222: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0037 - class_loss: 0.6880 - val_loss: 0.1402 - val_regression_loss: 8.7721e-04 - val_class_loss: 0.6965\n",
      "Epoch 224/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6885\n",
      "Epoch 00223: reducing learning rate to 0.0018000000854954123.\n",
      "Epoch 00223: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1409 - regression_loss: 0.0030 - class_loss: 0.6892 - val_loss: 0.1402 - val_regression_loss: 9.8092e-04 - val_class_loss: 0.6959\n",
      "Epoch 225/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1415 - regression_loss: 0.0034 - class_loss: 0.6903Epoch 00224: val_loss improved from 0.14000 to 0.13997, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1413 - regression_loss: 0.0034 - class_loss: 0.6899 - val_loss: 0.1400 - val_regression_loss: 8.9724e-04 - val_class_loss: 0.6954\n",
      "Epoch 226/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1434 - regression_loss: 0.0058 - class_loss: 0.6878Epoch 00225: val_loss improved from 0.13997 to 0.13983, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1432 - regression_loss: 0.0057 - class_loss: 0.6875 - val_loss: 0.1398 - val_regression_loss: 8.6655e-04 - val_class_loss: 0.6948\n",
      "Epoch 227/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0046 - class_loss: 0.6867Epoch 00226: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0047 - class_loss: 0.6870 - val_loss: 0.1412 - val_regression_loss: 0.0023 - val_class_loss: 0.6949\n",
      "Epoch 228/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0034 - class_loss: 0.6890Epoch 00227: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0034 - class_loss: 0.6886 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6957\n",
      "Epoch 229/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0029 - class_loss: 0.6882Epoch 00228: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0029 - class_loss: 0.6884 - val_loss: 0.1401 - val_regression_loss: 9.9794e-04 - val_class_loss: 0.6957\n",
      "Epoch 230/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0048 - class_loss: 0.6886Epoch 00229: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1424 - regression_loss: 0.0047 - class_loss: 0.6884 - val_loss: 0.1400 - val_regression_loss: 8.4026e-04 - val_class_loss: 0.6957\n",
      "Epoch 231/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1413 - regression_loss: 0.0036 - class_loss: 0.6885Epoch 00230: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1413 - regression_loss: 0.0036 - class_loss: 0.6888 - val_loss: 0.1400 - val_regression_loss: 8.5485e-04 - val_class_loss: 0.6958\n",
      "Epoch 232/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0032 - class_loss: 0.6884Epoch 00231: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0033 - class_loss: 0.6886 - val_loss: 0.1405 - val_regression_loss: 0.0013 - val_class_loss: 0.6959\n",
      "Epoch 233/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0037 - class_loss: 0.6871Epoch 00232: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0037 - class_loss: 0.6875 - val_loss: 0.1400 - val_regression_loss: 8.9155e-04 - val_class_loss: 0.6957\n",
      "Epoch 234/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0033 - class_loss: 0.6877Epoch 00233: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0033 - class_loss: 0.6876 - val_loss: 0.1401 - val_regression_loss: 9.2534e-04 - val_class_loss: 0.6960\n",
      "Epoch 235/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1411 - regression_loss: 0.0032 - class_loss: 0.6896Epoch 00234: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0032 - class_loss: 0.6889 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6961\n",
      "Epoch 236/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0027 - class_loss: 0.6874Epoch 00235: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0027 - class_loss: 0.6871 - val_loss: 0.1401 - val_regression_loss: 9.1330e-04 - val_class_loss: 0.6960\n",
      "Epoch 237/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0030 - class_loss: 0.6881Epoch 00236: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0029 - class_loss: 0.6880 - val_loss: 0.1402 - val_regression_loss: 9.1987e-04 - val_class_loss: 0.6964\n",
      "Epoch 238/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0025 - class_loss: 0.6896Epoch 00237: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0025 - class_loss: 0.6892 - val_loss: 0.1404 - val_regression_loss: 0.0011 - val_class_loss: 0.6964\n",
      "Epoch 239/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0030 - class_loss: 0.6887Epoch 00238: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1407 - regression_loss: 0.0029 - class_loss: 0.6887 - val_loss: 0.1403 - val_regression_loss: 9.5722e-04 - val_class_loss: 0.6970\n",
      "Epoch 240/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0030 - class_loss: 0.6888Epoch 00239: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6885 - val_loss: 0.1404 - val_regression_loss: 0.0010 - val_class_loss: 0.6966\n",
      "Epoch 241/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0040 - class_loss: 0.6902Epoch 00240: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0040 - class_loss: 0.6905 - val_loss: 0.1402 - val_regression_loss: 0.0010 - val_class_loss: 0.6958\n",
      "Epoch 242/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1423 - regression_loss: 0.0049 - class_loss: 0.6871Epoch 00241: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0048 - class_loss: 0.6872 - val_loss: 0.1406 - val_regression_loss: 0.0014 - val_class_loss: 0.6957\n",
      "Epoch 243/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0031 - class_loss: 0.6880Epoch 00242: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0031 - class_loss: 0.6885 - val_loss: 0.1399 - val_regression_loss: 8.9966e-04 - val_class_loss: 0.6951\n",
      "Epoch 244/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1408 - regression_loss: 0.0034 - class_loss: 0.6871Epoch 00243: val_loss improved from 0.13983 to 0.13970, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0034 - class_loss: 0.6872 - val_loss: 0.1397 - val_regression_loss: 7.7972e-04 - val_class_loss: 0.6946\n",
      "Epoch 245/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0028 - class_loss: 0.6885Epoch 00244: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0028 - class_loss: 0.6885 - val_loss: 0.1399 - val_regression_loss: 9.1229e-04 - val_class_loss: 0.6951\n",
      "Epoch 246/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1411 - regression_loss: 0.0039 - class_loss: 0.6863Epoch 00245: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1411 - regression_loss: 0.0038 - class_loss: 0.6861 - val_loss: 0.1397 - val_regression_loss: 7.7847e-04 - val_class_loss: 0.6947\n",
      "Epoch 247/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0035 - class_loss: 0.6904Epoch 00246: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1413 - regression_loss: 0.0035 - class_loss: 0.6892 - val_loss: 0.1399 - val_regression_loss: 8.5639e-04 - val_class_loss: 0.6951\n",
      "Epoch 248/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1415 - regression_loss: 0.0033 - class_loss: 0.6908Epoch 00247: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0033 - class_loss: 0.6908 - val_loss: 0.1402 - val_regression_loss: 0.0011 - val_class_loss: 0.6956\n",
      "Epoch 249/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0035 - class_loss: 0.6892Epoch 00248: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0035 - class_loss: 0.6894 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6957\n",
      "Epoch 250/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1423 - regression_loss: 0.0047 - class_loss: 0.6879Epoch 00249: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0047 - class_loss: 0.6881 - val_loss: 0.1399 - val_regression_loss: 8.4974e-04 - val_class_loss: 0.6953\n",
      "Epoch 251/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0037 - class_loss: 0.6873Epoch 00250: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0037 - class_loss: 0.6875 - val_loss: 0.1398 - val_regression_loss: 7.8366e-04 - val_class_loss: 0.6950\n",
      "Epoch 252/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0033 - class_loss: 0.6877Epoch 00251: val_loss improved from 0.13970 to 0.13969, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0033 - class_loss: 0.6878 - val_loss: 0.1397 - val_regression_loss: 7.5436e-04 - val_class_loss: 0.6947\n",
      "Epoch 253/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0031 - class_loss: 0.6862Epoch 00252: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0031 - class_loss: 0.6871 - val_loss: 0.1398 - val_regression_loss: 8.6058e-04 - val_class_loss: 0.6945\n",
      "Epoch 254/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0029 - class_loss: 0.6895Epoch 00253: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1409 - regression_loss: 0.0029 - class_loss: 0.6899 - val_loss: 0.1398 - val_regression_loss: 9.1365e-04 - val_class_loss: 0.6946\n",
      "Epoch 255/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0028 - class_loss: 0.6897Epoch 00254: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0027 - class_loss: 0.6896 - val_loss: 0.1398 - val_regression_loss: 8.9113e-04 - val_class_loss: 0.6946\n",
      "Epoch 256/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0025 - class_loss: 0.6890Epoch 00255: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0030 - class_loss: 0.6886 - val_loss: 0.1410 - val_regression_loss: 0.0020 - val_class_loss: 0.6948\n",
      "Epoch 257/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0029 - class_loss: 0.6888Epoch 00256: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0029 - class_loss: 0.6888 - val_loss: 0.1398 - val_regression_loss: 8.7704e-04 - val_class_loss: 0.6947\n",
      "Epoch 258/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1501 - regression_loss: 0.0124 - class_loss: 0.6888Epoch 00257: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1500 - regression_loss: 0.0122 - class_loss: 0.6889 - val_loss: 0.1399 - val_regression_loss: 9.6833e-04 - val_class_loss: 0.6946\n",
      "Epoch 259/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0027 - class_loss: 0.6900Epoch 00258: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0027 - class_loss: 0.6902 - val_loss: 0.1397 - val_regression_loss: 7.7340e-04 - val_class_loss: 0.6947\n",
      "Epoch 260/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0039 - class_loss: 0.6897Epoch 00259: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1418 - regression_loss: 0.0039 - class_loss: 0.6892 - val_loss: 0.1397 - val_regression_loss: 7.0324e-04 - val_class_loss: 0.6952\n",
      "Epoch 261/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0030 - class_loss: 0.6880Epoch 00260: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0030 - class_loss: 0.6878 - val_loss: 0.1400 - val_regression_loss: 8.4920e-04 - val_class_loss: 0.6956\n",
      "Epoch 262/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1484 - regression_loss: 0.0108 - class_loss: 0.6876Epoch 00261: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1482 - regression_loss: 0.0106 - class_loss: 0.6881 - val_loss: 0.1399 - val_regression_loss: 7.8622e-04 - val_class_loss: 0.6955\n",
      "Epoch 263/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0025 - class_loss: 0.6892Epoch 00262: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0024 - class_loss: 0.6891 - val_loss: 0.1398 - val_regression_loss: 7.9267e-04 - val_class_loss: 0.6952\n",
      "Epoch 264/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1479 - regression_loss: 0.0100 - class_loss: 0.6896Epoch 00263: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1476 - regression_loss: 0.0098 - class_loss: 0.6893 - val_loss: 0.1398 - val_regression_loss: 7.8812e-04 - val_class_loss: 0.6951\n",
      "Epoch 265/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1417 - regression_loss: 0.0042 - class_loss: 0.6877Epoch 00264: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1416 - regression_loss: 0.0041 - class_loss: 0.6877 - val_loss: 0.1401 - val_regression_loss: 0.0011 - val_class_loss: 0.6950\n",
      "Epoch 266/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0047 - class_loss: 0.6847Epoch 00265: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0046 - class_loss: 0.6843 - val_loss: 0.1397 - val_regression_loss: 7.5383e-04 - val_class_loss: 0.6948\n",
      "Epoch 267/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1421 - regression_loss: 0.0047 - class_loss: 0.6869Epoch 00266: val_loss improved from 0.13969 to 0.13963, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0046 - class_loss: 0.6873 - val_loss: 0.1396 - val_regression_loss: 8.5616e-04 - val_class_loss: 0.6939\n",
      "Epoch 268/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1427 - regression_loss: 0.0049 - class_loss: 0.6892Epoch 00267: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1426 - regression_loss: 0.0048 - class_loss: 0.6890 - val_loss: 0.1398 - val_regression_loss: 9.7207e-04 - val_class_loss: 0.6943\n",
      "Epoch 269/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1420 - regression_loss: 0.0040 - class_loss: 0.6901Epoch 00268: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0040 - class_loss: 0.6901 - val_loss: 0.1398 - val_regression_loss: 8.5524e-04 - val_class_loss: 0.6947\n",
      "Epoch 270/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0029 - class_loss: 0.6869Epoch 00269: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0029 - class_loss: 0.6874 - val_loss: 0.1397 - val_regression_loss: 7.2344e-04 - val_class_loss: 0.6951\n",
      "Epoch 271/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0026 - class_loss: 0.6905Epoch 00270: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0025 - class_loss: 0.6900 - val_loss: 0.1401 - val_regression_loss: 8.3593e-04 - val_class_loss: 0.6962\n",
      "Epoch 272/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0030 - class_loss: 0.6852Epoch 00271: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0030 - class_loss: 0.6853 - val_loss: 0.1401 - val_regression_loss: 8.5567e-04 - val_class_loss: 0.6964\n",
      "Epoch 273/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1418 - regression_loss: 0.0038 - class_loss: 0.6898Epoch 00272: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1417 - regression_loss: 0.0038 - class_loss: 0.6894 - val_loss: 0.1400 - val_regression_loss: 6.7991e-04 - val_class_loss: 0.6965\n",
      "Epoch 274/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1422 - regression_loss: 0.0044 - class_loss: 0.6889Epoch 00273: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0044 - class_loss: 0.6884 - val_loss: 0.1401 - val_regression_loss: 7.1285e-04 - val_class_loss: 0.6967\n",
      "Epoch 275/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1555 - regression_loss: 0.0176 - class_loss: 0.6896Epoch 00274: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1551 - regression_loss: 0.0172 - class_loss: 0.6893 - val_loss: 0.1400 - val_regression_loss: 6.9650e-04 - val_class_loss: 0.6963\n",
      "Epoch 276/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1413 - regression_loss: 0.0031 - class_loss: 0.6907Epoch 00275: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1411 - regression_loss: 0.0031 - class_loss: 0.6901 - val_loss: 0.1400 - val_regression_loss: 6.3793e-04 - val_class_loss: 0.6966\n",
      "Epoch 277/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0030 - class_loss: 0.6861Epoch 00276: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0030 - class_loss: 0.6864 - val_loss: 0.1400 - val_regression_loss: 7.0974e-04 - val_class_loss: 0.6964\n",
      "Epoch 278/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0049 - class_loss: 0.6881Epoch 00277: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0048 - class_loss: 0.6878 - val_loss: 0.1399 - val_regression_loss: 7.5144e-04 - val_class_loss: 0.6959\n",
      "Epoch 279/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0032 - class_loss: 0.6875Epoch 00278: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0032 - class_loss: 0.6870 - val_loss: 0.1397 - val_regression_loss: 7.2795e-04 - val_class_loss: 0.6949\n",
      "Epoch 280/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0033 - class_loss: 0.6866Epoch 00279: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1412 - regression_loss: 0.0039 - class_loss: 0.6865 - val_loss: 0.1441 - val_regression_loss: 0.0052 - val_class_loss: 0.6948\n",
      "Epoch 281/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1450 - regression_loss: 0.0072 - class_loss: 0.6887Epoch 00280: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1448 - regression_loss: 0.0071 - class_loss: 0.6886 - val_loss: 0.1398 - val_regression_loss: 7.5946e-04 - val_class_loss: 0.6953\n",
      "Epoch 282/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1422 - regression_loss: 0.0044 - class_loss: 0.6889Epoch 00281: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1420 - regression_loss: 0.0044 - class_loss: 0.6882 - val_loss: 0.1409 - val_regression_loss: 0.0017 - val_class_loss: 0.6960\n",
      "Epoch 283/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0033 - class_loss: 0.6876Epoch 00282: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0034 - class_loss: 0.6873 - val_loss: 0.1399 - val_regression_loss: 7.8204e-04 - val_class_loss: 0.6958\n",
      "Epoch 284/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1401 - regression_loss: 0.0030 - class_loss: 0.6857Epoch 00283: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0029 - class_loss: 0.6858 - val_loss: 0.1401 - val_regression_loss: 8.8153e-04 - val_class_loss: 0.6960\n",
      "Epoch 285/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0026 - class_loss: 0.6853Epoch 00284: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0026 - class_loss: 0.6862 - val_loss: 0.1398 - val_regression_loss: 8.5368e-04 - val_class_loss: 0.6945\n",
      "Epoch 286/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0027 - class_loss: 0.6874Epoch 00285: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0027 - class_loss: 0.6879 - val_loss: 0.1396 - val_regression_loss: 9.0690e-04 - val_class_loss: 0.6937\n",
      "Epoch 287/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0035 - class_loss: 0.6872Epoch 00286: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0035 - class_loss: 0.6874 - val_loss: 0.1401 - val_regression_loss: 0.0014 - val_class_loss: 0.6935\n",
      "Epoch 288/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0026 - class_loss: 0.6900Epoch 00287: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0026 - class_loss: 0.6900 - val_loss: 0.1400 - val_regression_loss: 0.0011 - val_class_loss: 0.6945\n",
      "Epoch 289/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0031 - class_loss: 0.6867Epoch 00288: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0031 - class_loss: 0.6874 - val_loss: 0.1398 - val_regression_loss: 8.9796e-04 - val_class_loss: 0.6946\n",
      "Epoch 290/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0031 - class_loss: 0.6868Epoch 00289: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0031 - class_loss: 0.6868 - val_loss: 0.1397 - val_regression_loss: 8.8068e-04 - val_class_loss: 0.6940\n",
      "Epoch 291/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0033 - class_loss: 0.6905Epoch 00290: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1414 - regression_loss: 0.0033 - class_loss: 0.6903 - val_loss: 0.1397 - val_regression_loss: 9.1739e-04 - val_class_loss: 0.6941\n",
      "Epoch 292/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1501 - regression_loss: 0.0125 - class_loss: 0.6881Epoch 00291: val_loss improved from 0.13963 to 0.13961, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1499 - regression_loss: 0.0122 - class_loss: 0.6883 - val_loss: 0.1396 - val_regression_loss: 7.3822e-04 - val_class_loss: 0.6944\n",
      "Epoch 293/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1401 - regression_loss: 0.0027 - class_loss: 0.6869Epoch 00292: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0027 - class_loss: 0.6875 - val_loss: 0.1396 - val_regression_loss: 7.0581e-04 - val_class_loss: 0.6945\n",
      "Epoch 294/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0045 - class_loss: 0.6871Epoch 00293: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1419 - regression_loss: 0.0045 - class_loss: 0.6870 - val_loss: 0.1400 - val_regression_loss: 0.0011 - val_class_loss: 0.6947\n",
      "Epoch 295/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0031 - class_loss: 0.6889\n",
      "Epoch 00294: reducing learning rate to 0.0016200000769458712.\n",
      "Epoch 00294: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0031 - class_loss: 0.6891 - val_loss: 0.1400 - val_regression_loss: 0.0011 - val_class_loss: 0.6949\n",
      "Epoch 296/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1401 - regression_loss: 0.0028 - class_loss: 0.6864Epoch 00295: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0028 - class_loss: 0.6869 - val_loss: 0.1396 - val_regression_loss: 7.0505e-04 - val_class_loss: 0.6946\n",
      "Epoch 297/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0028 - class_loss: 0.6893Epoch 00296: val_loss improved from 0.13961 to 0.13960, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1407 - regression_loss: 0.0027 - class_loss: 0.6896 - val_loss: 0.1396 - val_regression_loss: 6.9098e-04 - val_class_loss: 0.6945\n",
      "Epoch 298/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1432 - regression_loss: 0.0058 - class_loss: 0.6874Epoch 00297: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1431 - regression_loss: 0.0056 - class_loss: 0.6873 - val_loss: 0.1399 - val_regression_loss: 9.1604e-04 - val_class_loss: 0.6949\n",
      "Epoch 299/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0025 - class_loss: 0.6883Epoch 00298: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0025 - class_loss: 0.6882 - val_loss: 0.1396 - val_regression_loss: 6.5668e-04 - val_class_loss: 0.6949\n",
      "Epoch 300/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1415 - regression_loss: 0.0037 - class_loss: 0.6889Epoch 00299: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1413 - regression_loss: 0.0036 - class_loss: 0.6882 - val_loss: 0.1397 - val_regression_loss: 6.8786e-04 - val_class_loss: 0.6953\n",
      "Epoch 301/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0029 - class_loss: 0.6863Epoch 00300: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0029 - class_loss: 0.6867 - val_loss: 0.1398 - val_regression_loss: 8.1614e-04 - val_class_loss: 0.6949\n",
      "Epoch 302/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1460 - regression_loss: 0.0086 - class_loss: 0.6869Epoch 00301: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1458 - regression_loss: 0.0084 - class_loss: 0.6869 - val_loss: 0.1398 - val_regression_loss: 8.0851e-04 - val_class_loss: 0.6947\n",
      "Epoch 303/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1411 - regression_loss: 0.0034 - class_loss: 0.6883Epoch 00302: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1411 - regression_loss: 0.0034 - class_loss: 0.6884 - val_loss: 0.1396 - val_regression_loss: 6.9954e-04 - val_class_loss: 0.6945\n",
      "Epoch 304/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0031 - class_loss: 0.6869Epoch 00303: val_loss improved from 0.13960 to 0.13957, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0031 - class_loss: 0.6870 - val_loss: 0.1396 - val_regression_loss: 7.1730e-04 - val_class_loss: 0.6943\n",
      "Epoch 305/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1401 - regression_loss: 0.0024 - class_loss: 0.6883Epoch 00304: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0025 - class_loss: 0.6882 - val_loss: 0.1398 - val_regression_loss: 8.3512e-04 - val_class_loss: 0.6946\n",
      "Epoch 306/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0023 - class_loss: 0.6864Epoch 00305: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0023 - class_loss: 0.6860 - val_loss: 0.1398 - val_regression_loss: 6.9709e-04 - val_class_loss: 0.6955\n",
      "Epoch 307/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1401 - regression_loss: 0.0027 - class_loss: 0.6872Epoch 00306: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0028 - class_loss: 0.6868 - val_loss: 0.1411 - val_regression_loss: 0.0018 - val_class_loss: 0.6965\n",
      "Epoch 308/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1431 - regression_loss: 0.0052 - class_loss: 0.6894Epoch 00307: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1430 - regression_loss: 0.0052 - class_loss: 0.6892 - val_loss: 0.1403 - val_regression_loss: 0.0011 - val_class_loss: 0.6962\n",
      "Epoch 309/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0030 - class_loss: 0.6843Epoch 00308: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0030 - class_loss: 0.6843 - val_loss: 0.1400 - val_regression_loss: 8.7736e-04 - val_class_loss: 0.6959\n",
      "Epoch 310/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0026 - class_loss: 0.6882Epoch 00309: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0026 - class_loss: 0.6884 - val_loss: 0.1398 - val_regression_loss: 7.9449e-04 - val_class_loss: 0.6948\n",
      "Epoch 311/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0031 - class_loss: 0.6870Epoch 00310: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0031 - class_loss: 0.6874 - val_loss: 0.1397 - val_regression_loss: 7.8246e-04 - val_class_loss: 0.6948\n",
      "Epoch 312/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1426 - regression_loss: 0.0051 - class_loss: 0.6879Epoch 00311: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1426 - regression_loss: 0.0050 - class_loss: 0.6881 - val_loss: 0.1401 - val_regression_loss: 0.0011 - val_class_loss: 0.6948\n",
      "Epoch 313/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0025 - class_loss: 0.6898Epoch 00312: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0025 - class_loss: 0.6891 - val_loss: 0.1400 - val_regression_loss: 9.1237e-04 - val_class_loss: 0.6952\n",
      "Epoch 314/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0027 - class_loss: 0.6878Epoch 00313: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0027 - class_loss: 0.6873 - val_loss: 0.1399 - val_regression_loss: 7.9871e-04 - val_class_loss: 0.6957\n",
      "Epoch 315/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0030 - class_loss: 0.6865Epoch 00314: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0030 - class_loss: 0.6869 - val_loss: 0.1403 - val_regression_loss: 0.0012 - val_class_loss: 0.6956\n",
      "Epoch 316/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0027 - class_loss: 0.6874Epoch 00315: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0027 - class_loss: 0.6864 - val_loss: 0.1397 - val_regression_loss: 6.7472e-04 - val_class_loss: 0.6954\n",
      "Epoch 317/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1494 - regression_loss: 0.0121 - class_loss: 0.6864Epoch 00316: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1495 - regression_loss: 0.0122 - class_loss: 0.6865 - val_loss: 0.1404 - val_regression_loss: 0.0012 - val_class_loss: 0.6956\n",
      "Epoch 318/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0027 - class_loss: 0.6851Epoch 00317: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1397 - regression_loss: 0.0027 - class_loss: 0.6853 - val_loss: 0.1400 - val_regression_loss: 0.0010 - val_class_loss: 0.6952\n",
      "Epoch 319/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1409 - regression_loss: 0.0034 - class_loss: 0.6878Epoch 00318: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1408 - regression_loss: 0.0033 - class_loss: 0.6876 - val_loss: 0.1398 - val_regression_loss: 7.7044e-04 - val_class_loss: 0.6952\n",
      "Epoch 320/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1393 - regression_loss: 0.0022 - class_loss: 0.6857Epoch 00319: val_loss improved from 0.13957 to 0.13951, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0022 - class_loss: 0.6867 - val_loss: 0.1395 - val_regression_loss: 7.3213e-04 - val_class_loss: 0.6939\n",
      "Epoch 321/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0026 - class_loss: 0.6864Epoch 00320: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0026 - class_loss: 0.6867 - val_loss: 0.1395 - val_regression_loss: 7.7819e-04 - val_class_loss: 0.6938\n",
      "Epoch 322/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0027 - class_loss: 0.6885Epoch 00321: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0027 - class_loss: 0.6887 - val_loss: 0.1396 - val_regression_loss: 7.3296e-04 - val_class_loss: 0.6944\n",
      "Epoch 323/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1395 - regression_loss: 0.0025 - class_loss: 0.6850Epoch 00322: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0026 - class_loss: 0.6852 - val_loss: 0.1396 - val_regression_loss: 7.0151e-04 - val_class_loss: 0.6945\n",
      "Epoch 324/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0033 - class_loss: 0.6856Epoch 00323: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0033 - class_loss: 0.6861 - val_loss: 0.1396 - val_regression_loss: 7.4258e-04 - val_class_loss: 0.6942\n",
      "Epoch 325/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1398 - regression_loss: 0.0026 - class_loss: 0.6861Epoch 00324: val_loss improved from 0.13951 to 0.13940, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1398 - regression_loss: 0.0026 - class_loss: 0.6863 - val_loss: 0.1394 - val_regression_loss: 6.5853e-04 - val_class_loss: 0.6937\n",
      "Epoch 326/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1393 - regression_loss: 0.0021 - class_loss: 0.6861Epoch 00325: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0021 - class_loss: 0.6863 - val_loss: 0.1395 - val_regression_loss: 7.5175e-04 - val_class_loss: 0.6936\n",
      "Epoch 327/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1394 - regression_loss: 0.0022 - class_loss: 0.6860Epoch 00326: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0022 - class_loss: 0.6854 - val_loss: 0.1394 - val_regression_loss: 6.8082e-04 - val_class_loss: 0.6937\n",
      "Epoch 328/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0028 - class_loss: 0.6866Epoch 00327: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0028 - class_loss: 0.6865 - val_loss: 0.1397 - val_regression_loss: 8.1879e-04 - val_class_loss: 0.6944\n",
      "Epoch 329/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1401 - regression_loss: 0.0030 - class_loss: 0.6856Epoch 00328: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0030 - class_loss: 0.6863 - val_loss: 0.1396 - val_regression_loss: 7.3723e-04 - val_class_loss: 0.6943\n",
      "Epoch 330/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0025 - class_loss: 0.6856Epoch 00329: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0025 - class_loss: 0.6857 - val_loss: 0.1400 - val_regression_loss: 8.7647e-04 - val_class_loss: 0.6954\n",
      "Epoch 331/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1459 - regression_loss: 0.0088 - class_loss: 0.6857Epoch 00330: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1456 - regression_loss: 0.0086 - class_loss: 0.6852 - val_loss: 0.1401 - val_regression_loss: 8.4947e-04 - val_class_loss: 0.6963\n",
      "Epoch 332/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1444 - regression_loss: 0.0063 - class_loss: 0.6903Epoch 00331: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1441 - regression_loss: 0.0062 - class_loss: 0.6896 - val_loss: 0.1402 - val_regression_loss: 9.8196e-04 - val_class_loss: 0.6963\n",
      "Epoch 333/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1398 - regression_loss: 0.0025 - class_loss: 0.6866Epoch 00332: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1398 - regression_loss: 0.0024 - class_loss: 0.6868 - val_loss: 0.1401 - val_regression_loss: 8.4713e-04 - val_class_loss: 0.6962\n",
      "Epoch 334/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1398 - regression_loss: 0.0024 - class_loss: 0.6868Epoch 00333: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1397 - regression_loss: 0.0024 - class_loss: 0.6865 - val_loss: 0.1399 - val_regression_loss: 7.0636e-04 - val_class_loss: 0.6959\n",
      "Epoch 335/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0023 - class_loss: 0.6882Epoch 00334: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0023 - class_loss: 0.6880 - val_loss: 0.1400 - val_regression_loss: 7.9150e-04 - val_class_loss: 0.6960\n",
      "Epoch 336/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1498 - regression_loss: 0.0125 - class_loss: 0.6862Epoch 00335: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1495 - regression_loss: 0.0122 - class_loss: 0.6865 - val_loss: 0.1399 - val_regression_loss: 8.2497e-04 - val_class_loss: 0.6954\n",
      "Epoch 337/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1412 - regression_loss: 0.0036 - class_loss: 0.6879Epoch 00336: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0036 - class_loss: 0.6872 - val_loss: 0.1396 - val_regression_loss: 6.6099e-04 - val_class_loss: 0.6947\n",
      "Epoch 338/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1423 - regression_loss: 0.0047 - class_loss: 0.6878Epoch 00337: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0047 - class_loss: 0.6885 - val_loss: 0.1395 - val_regression_loss: 6.1305e-04 - val_class_loss: 0.6942\n",
      "Epoch 339/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1492 - regression_loss: 0.0122 - class_loss: 0.6848Epoch 00338: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1488 - regression_loss: 0.0119 - class_loss: 0.6844 - val_loss: 0.1395 - val_regression_loss: 6.3834e-04 - val_class_loss: 0.6943\n",
      "Epoch 340/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0022 - class_loss: 0.6870Epoch 00339: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1398 - regression_loss: 0.0022 - class_loss: 0.6880 - val_loss: 0.1395 - val_regression_loss: 6.7616e-04 - val_class_loss: 0.6941\n",
      "Epoch 341/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0030 - class_loss: 0.6865Epoch 00340: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0030 - class_loss: 0.6863 - val_loss: 0.1394 - val_regression_loss: 6.1087e-04 - val_class_loss: 0.6942\n",
      "Epoch 342/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0024 - class_loss: 0.6865Epoch 00341: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0024 - class_loss: 0.6862 - val_loss: 0.1395 - val_regression_loss: 6.1282e-04 - val_class_loss: 0.6946\n",
      "Epoch 343/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1392 - regression_loss: 0.0022 - class_loss: 0.6850Epoch 00342: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0022 - class_loss: 0.6854 - val_loss: 0.1395 - val_regression_loss: 6.2947e-04 - val_class_loss: 0.6944\n",
      "Epoch 344/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0027 - class_loss: 0.6867Epoch 00343: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0027 - class_loss: 0.6874 - val_loss: 0.1397 - val_regression_loss: 9.2529e-04 - val_class_loss: 0.6940\n",
      "Epoch 345/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1490 - regression_loss: 0.0116 - class_loss: 0.6870Epoch 00344: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1489 - regression_loss: 0.0115 - class_loss: 0.6868 - val_loss: 0.1402 - val_regression_loss: 0.0014 - val_class_loss: 0.6941\n",
      "Epoch 346/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1444 - regression_loss: 0.0069 - class_loss: 0.6876Epoch 00345: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1443 - regression_loss: 0.0067 - class_loss: 0.6878 - val_loss: 0.1396 - val_regression_loss: 6.6907e-04 - val_class_loss: 0.6945\n",
      "Epoch 347/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1506 - regression_loss: 0.0133 - class_loss: 0.6862Epoch 00346: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1501 - regression_loss: 0.0130 - class_loss: 0.6857 - val_loss: 0.1394 - val_regression_loss: 5.4530e-04 - val_class_loss: 0.6944\n",
      "Epoch 348/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0029 - class_loss: 0.6875Epoch 00347: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0029 - class_loss: 0.6876 - val_loss: 0.1394 - val_regression_loss: 6.6742e-04 - val_class_loss: 0.6938\n",
      "Epoch 349/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1414 - regression_loss: 0.0039 - class_loss: 0.6873Epoch 00348: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1423 - regression_loss: 0.0049 - class_loss: 0.6870 - val_loss: 0.1500 - val_regression_loss: 0.0114 - val_class_loss: 0.6934\n",
      "Epoch 350/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1453 - regression_loss: 0.0082 - class_loss: 0.6855Epoch 00349: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1450 - regression_loss: 0.0080 - class_loss: 0.6851 - val_loss: 0.1397 - val_regression_loss: 8.9451e-04 - val_class_loss: 0.6941\n",
      "Epoch 351/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0035 - class_loss: 0.6855Epoch 00350: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0035 - class_loss: 0.6853 - val_loss: 0.1400 - val_regression_loss: 0.0010 - val_class_loss: 0.6950\n",
      "Epoch 352/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0029 - class_loss: 0.6856Epoch 00351: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1402 - regression_loss: 0.0029 - class_loss: 0.6861 - val_loss: 0.1398 - val_regression_loss: 9.2076e-04 - val_class_loss: 0.6946\n",
      "Epoch 353/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1394 - regression_loss: 0.0025 - class_loss: 0.6841Epoch 00352: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0026 - class_loss: 0.6844 - val_loss: 0.1397 - val_regression_loss: 9.1607e-04 - val_class_loss: 0.6938\n",
      "Epoch 354/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0027 - class_loss: 0.6887Epoch 00353: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1404 - regression_loss: 0.0027 - class_loss: 0.6887 - val_loss: 0.1394 - val_regression_loss: 7.9812e-04 - val_class_loss: 0.6933\n",
      "Epoch 355/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1390 - regression_loss: 0.0023 - class_loss: 0.6837Epoch 00354: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0027 - class_loss: 0.6842 - val_loss: 0.1406 - val_regression_loss: 0.0020 - val_class_loss: 0.6930\n",
      "Epoch 356/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0032 - class_loss: 0.6837Epoch 00355: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1398 - regression_loss: 0.0031 - class_loss: 0.6837 - val_loss: 0.1394 - val_regression_loss: 6.9901e-04 - val_class_loss: 0.6935\n",
      "Epoch 357/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1480 - regression_loss: 0.0112 - class_loss: 0.6840Epoch 00356: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1477 - regression_loss: 0.0110 - class_loss: 0.6837 - val_loss: 0.1396 - val_regression_loss: 8.2605e-04 - val_class_loss: 0.6937\n",
      "Epoch 358/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1404 - regression_loss: 0.0029 - class_loss: 0.6873Epoch 00357: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0029 - class_loss: 0.6872 - val_loss: 0.1396 - val_regression_loss: 7.2310e-04 - val_class_loss: 0.6943\n",
      "Epoch 359/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1395 - regression_loss: 0.0029 - class_loss: 0.6831Epoch 00358: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0028 - class_loss: 0.6836 - val_loss: 0.1399 - val_regression_loss: 0.0011 - val_class_loss: 0.6939\n",
      "Epoch 360/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0036 - class_loss: 0.6849Epoch 00359: val_loss improved from 0.13940 to 0.13935, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1407 - regression_loss: 0.0036 - class_loss: 0.6856 - val_loss: 0.1394 - val_regression_loss: 6.4776e-04 - val_class_loss: 0.6935\n",
      "Epoch 361/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1393 - regression_loss: 0.0021 - class_loss: 0.6858Epoch 00360: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0022 - class_loss: 0.6861 - val_loss: 0.1396 - val_regression_loss: 9.7508e-04 - val_class_loss: 0.6929\n",
      "Epoch 362/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0028 - class_loss: 0.6839Epoch 00361: val_loss improved from 0.13935 to 0.13925, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0027 - class_loss: 0.6843 - val_loss: 0.1393 - val_regression_loss: 6.1640e-04 - val_class_loss: 0.6932\n",
      "Epoch 363/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0025 - class_loss: 0.6868Epoch 00362: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0028 - class_loss: 0.6865 - val_loss: 0.1394 - val_regression_loss: 6.9869e-04 - val_class_loss: 0.6935\n",
      "Epoch 364/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0025 - class_loss: 0.6860Epoch 00363: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1397 - regression_loss: 0.0025 - class_loss: 0.6860 - val_loss: 0.1395 - val_regression_loss: 7.2832e-04 - val_class_loss: 0.6941\n",
      "Epoch 365/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1394 - regression_loss: 0.0025 - class_loss: 0.6843Epoch 00364: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0026 - class_loss: 0.6837 - val_loss: 0.1395 - val_regression_loss: 6.5898e-04 - val_class_loss: 0.6944\n",
      "Epoch 366/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1462 - regression_loss: 0.0090 - class_loss: 0.6863Epoch 00365: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1461 - regression_loss: 0.0088 - class_loss: 0.6867 - val_loss: 0.1397 - val_regression_loss: 8.1137e-04 - val_class_loss: 0.6943\n",
      "Epoch 367/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1403 - regression_loss: 0.0033 - class_loss: 0.6854Epoch 00366: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0033 - class_loss: 0.6861 - val_loss: 0.1397 - val_regression_loss: 9.7292e-04 - val_class_loss: 0.6938\n",
      "Epoch 368/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1415 - regression_loss: 0.0044 - class_loss: 0.6854Epoch 00367: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1413 - regression_loss: 0.0043 - class_loss: 0.6850 - val_loss: 0.1395 - val_regression_loss: 6.6803e-04 - val_class_loss: 0.6943\n",
      "Epoch 369/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0028 - class_loss: 0.6848Epoch 00368: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1397 - regression_loss: 0.0027 - class_loss: 0.6851 - val_loss: 0.1396 - val_regression_loss: 7.8921e-04 - val_class_loss: 0.6943\n",
      "Epoch 370/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1416 - regression_loss: 0.0039 - class_loss: 0.6886Epoch 00369: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1416 - regression_loss: 0.0038 - class_loss: 0.6887 - val_loss: 0.1395 - val_regression_loss: 6.3334e-04 - val_class_loss: 0.6945\n",
      "Epoch 371/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1394 - regression_loss: 0.0025 - class_loss: 0.6847Epoch 00370: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0024 - class_loss: 0.6844 - val_loss: 0.1395 - val_regression_loss: 6.1762e-04 - val_class_loss: 0.6942\n",
      "Epoch 372/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0032 - class_loss: 0.6836Epoch 00371: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0032 - class_loss: 0.6835 - val_loss: 0.1397 - val_regression_loss: 7.4659e-04 - val_class_loss: 0.6948\n",
      "Epoch 373/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0023 - class_loss: 0.6870Epoch 00372: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1397 - regression_loss: 0.0024 - class_loss: 0.6867 - val_loss: 0.1398 - val_regression_loss: 8.5128e-04 - val_class_loss: 0.6948\n",
      "Epoch 374/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0023 - class_loss: 0.6865Epoch 00373: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0023 - class_loss: 0.6863 - val_loss: 0.1396 - val_regression_loss: 7.4768e-04 - val_class_loss: 0.6945\n",
      "Epoch 375/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1474 - regression_loss: 0.0105 - class_loss: 0.6845Epoch 00374: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1471 - regression_loss: 0.0103 - class_loss: 0.6843 - val_loss: 0.1397 - val_regression_loss: 8.3087e-04 - val_class_loss: 0.6942\n",
      "Epoch 376/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0033 - class_loss: 0.6868Epoch 00375: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1407 - regression_loss: 0.0032 - class_loss: 0.6876 - val_loss: 0.1393 - val_regression_loss: 5.8106e-04 - val_class_loss: 0.6934\n",
      "Epoch 377/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1398 - regression_loss: 0.0022 - class_loss: 0.6879Epoch 00376: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0023 - class_loss: 0.6881 - val_loss: 0.1398 - val_regression_loss: 0.0010 - val_class_loss: 0.6939\n",
      "Epoch 378/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0031 - class_loss: 0.6854Epoch 00377: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0031 - class_loss: 0.6857 - val_loss: 0.1395 - val_regression_loss: 7.1587e-04 - val_class_loss: 0.6941\n",
      "Epoch 379/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0021 - class_loss: 0.6835Epoch 00378: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1389 - regression_loss: 0.0022 - class_loss: 0.6834 - val_loss: 0.1398 - val_regression_loss: 9.7558e-04 - val_class_loss: 0.6942\n",
      "Epoch 380/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0030 - class_loss: 0.6852Epoch 00379: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1400 - regression_loss: 0.0029 - class_loss: 0.6852 - val_loss: 0.1395 - val_regression_loss: 7.1432e-04 - val_class_loss: 0.6940\n",
      "Epoch 381/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0022 - class_loss: 0.6832Epoch 00380: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1389 - regression_loss: 0.0022 - class_loss: 0.6831 - val_loss: 0.1400 - val_regression_loss: 0.0011 - val_class_loss: 0.6942\n",
      "Epoch 382/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0033 - class_loss: 0.6866Epoch 00381: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0033 - class_loss: 0.6863 - val_loss: 0.1396 - val_regression_loss: 7.7413e-04 - val_class_loss: 0.6940\n",
      "Epoch 383/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1422 - regression_loss: 0.0051 - class_loss: 0.6860Epoch 00382: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1421 - regression_loss: 0.0049 - class_loss: 0.6858 - val_loss: 0.1396 - val_regression_loss: 7.3646e-04 - val_class_loss: 0.6942\n",
      "Epoch 384/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1487 - regression_loss: 0.0116 - class_loss: 0.6852Epoch 00383: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1484 - regression_loss: 0.0114 - class_loss: 0.6847 - val_loss: 0.1397 - val_regression_loss: 8.7675e-04 - val_class_loss: 0.6941\n",
      "Epoch 385/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1389 - regression_loss: 0.0020 - class_loss: 0.6843Epoch 00384: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1389 - regression_loss: 0.0020 - class_loss: 0.6843 - val_loss: 0.1395 - val_regression_loss: 6.6580e-04 - val_class_loss: 0.6943\n",
      "Epoch 386/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1389 - regression_loss: 0.0024 - class_loss: 0.6825Epoch 00385: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1388 - regression_loss: 0.0024 - class_loss: 0.6820 - val_loss: 0.1396 - val_regression_loss: 6.9419e-04 - val_class_loss: 0.6946\n",
      "Epoch 387/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0025 - class_loss: 0.6854Epoch 00386: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0025 - class_loss: 0.6846 - val_loss: 0.1399 - val_regression_loss: 7.1827e-04 - val_class_loss: 0.6958\n",
      "Epoch 388/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1391 - regression_loss: 0.0027 - class_loss: 0.6817Epoch 00387: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0027 - class_loss: 0.6813 - val_loss: 0.1401 - val_regression_loss: 9.0517e-04 - val_class_loss: 0.6959\n",
      "Epoch 389/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1447 - regression_loss: 0.0079 - class_loss: 0.6841Epoch 00388: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1446 - regression_loss: 0.0078 - class_loss: 0.6842 - val_loss: 0.1394 - val_regression_loss: 5.5707e-04 - val_class_loss: 0.6940\n",
      "Epoch 390/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1392 - regression_loss: 0.0024 - class_loss: 0.6837Epoch 00389: val_loss improved from 0.13925 to 0.13920, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1391 - regression_loss: 0.0024 - class_loss: 0.6836 - val_loss: 0.1392 - val_regression_loss: 6.5563e-04 - val_class_loss: 0.6927\n",
      "Epoch 391/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1410 - regression_loss: 0.0038 - class_loss: 0.6860Epoch 00390: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1406 - regression_loss: 0.0037 - class_loss: 0.6846 - val_loss: 0.1393 - val_regression_loss: 6.1082e-04 - val_class_loss: 0.6933\n",
      "Epoch 392/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0025 - class_loss: 0.6884Epoch 00391: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0025 - class_loss: 0.6881 - val_loss: 0.1394 - val_regression_loss: 6.4551e-04 - val_class_loss: 0.6938\n",
      "Epoch 393/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1407 - regression_loss: 0.0033 - class_loss: 0.6870Epoch 00392: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0037 - class_loss: 0.6867 - val_loss: 0.1402 - val_regression_loss: 0.0013 - val_class_loss: 0.6944\n",
      "Epoch 394/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1431 - regression_loss: 0.0058 - class_loss: 0.6865Epoch 00393: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1431 - regression_loss: 0.0057 - class_loss: 0.6870 - val_loss: 0.1395 - val_regression_loss: 6.2752e-04 - val_class_loss: 0.6945\n",
      "Epoch 395/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1389 - regression_loss: 0.0023 - class_loss: 0.6834Epoch 00394: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1389 - regression_loss: 0.0022 - class_loss: 0.6834 - val_loss: 0.1394 - val_regression_loss: 6.6760e-04 - val_class_loss: 0.6938\n",
      "Epoch 396/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0032 - class_loss: 0.6837Epoch 00395: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0032 - class_loss: 0.6836 - val_loss: 0.1395 - val_regression_loss: 6.8838e-04 - val_class_loss: 0.6939\n",
      "Epoch 397/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1389 - regression_loss: 0.0022 - class_loss: 0.6838Epoch 00396: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0027 - class_loss: 0.6843 - val_loss: 0.1417 - val_regression_loss: 0.0030 - val_class_loss: 0.6934\n",
      "Epoch 398/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0029 - class_loss: 0.6851Epoch 00397: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0029 - class_loss: 0.6851 - val_loss: 0.1395 - val_regression_loss: 7.6430e-04 - val_class_loss: 0.6937\n",
      "Epoch 399/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1394 - regression_loss: 0.0027 - class_loss: 0.6831Epoch 00398: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0027 - class_loss: 0.6828 - val_loss: 0.1395 - val_regression_loss: 6.6009e-04 - val_class_loss: 0.6942\n",
      "Epoch 400/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0027 - class_loss: 0.6805Epoch 00399: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1410 - regression_loss: 0.0048 - class_loss: 0.6810 - val_loss: 0.1422 - val_regression_loss: 0.0034 - val_class_loss: 0.6943\n",
      "Epoch 401/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1398 - regression_loss: 0.0031 - class_loss: 0.6837Epoch 00400: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1398 - regression_loss: 0.0031 - class_loss: 0.6835 - val_loss: 0.1394 - val_regression_loss: 8.2816e-04 - val_class_loss: 0.6929\n",
      "Epoch 402/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1486 - regression_loss: 0.0116 - class_loss: 0.6848Epoch 00401: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1483 - regression_loss: 0.0113 - class_loss: 0.6847 - val_loss: 0.1393 - val_regression_loss: 7.1287e-04 - val_class_loss: 0.6928\n",
      "Epoch 403/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0033 - class_loss: 0.6819Epoch 00402: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0032 - class_loss: 0.6818 - val_loss: 0.1394 - val_regression_loss: 6.5039e-04 - val_class_loss: 0.6939\n",
      "Epoch 404/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1459 - regression_loss: 0.0092 - class_loss: 0.6833Epoch 00403: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1456 - regression_loss: 0.0090 - class_loss: 0.6829 - val_loss: 0.1394 - val_regression_loss: 5.8538e-04 - val_class_loss: 0.6943\n",
      "Epoch 405/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1384 - regression_loss: 0.0022 - class_loss: 0.6810Epoch 00404: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0022 - class_loss: 0.6810 - val_loss: 0.1396 - val_regression_loss: 5.8218e-04 - val_class_loss: 0.6951\n",
      "Epoch 406/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1398 - regression_loss: 0.0031 - class_loss: 0.6836Epoch 00405: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1463 - regression_loss: 0.0097 - class_loss: 0.6832 - val_loss: 0.1639 - val_regression_loss: 0.0252 - val_class_loss: 0.6936\n",
      "Epoch 407/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1483 - regression_loss: 0.0118 - class_loss: 0.6829Epoch 00406: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1482 - regression_loss: 0.0117 - class_loss: 0.6823 - val_loss: 0.1398 - val_regression_loss: 9.8677e-04 - val_class_loss: 0.6943\n",
      "Epoch 408/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1391 - regression_loss: 0.0025 - class_loss: 0.6830Epoch 00407: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1388 - regression_loss: 0.0025 - class_loss: 0.6817 - val_loss: 0.1400 - val_regression_loss: 9.5524e-04 - val_class_loss: 0.6952\n",
      "Epoch 409/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1473 - regression_loss: 0.0104 - class_loss: 0.6842Epoch 00408: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1470 - regression_loss: 0.0102 - class_loss: 0.6841 - val_loss: 0.1399 - val_regression_loss: 7.1632e-04 - val_class_loss: 0.6958\n",
      "Epoch 410/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1400 - regression_loss: 0.0032 - class_loss: 0.6841Epoch 00409: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1400 - regression_loss: 0.0032 - class_loss: 0.6840 - val_loss: 0.1399 - val_regression_loss: 6.8614e-04 - val_class_loss: 0.6962\n",
      "Epoch 411/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1512 - regression_loss: 0.0134 - class_loss: 0.6892Epoch 00410: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1507 - regression_loss: 0.0131 - class_loss: 0.6883 - val_loss: 0.1397 - val_regression_loss: 6.2609e-04 - val_class_loss: 0.6956\n",
      "Epoch 412/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1395 - regression_loss: 0.0024 - class_loss: 0.6853Epoch 00411: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0024 - class_loss: 0.6859 - val_loss: 0.1396 - val_regression_loss: 7.1829e-04 - val_class_loss: 0.6943\n",
      "Epoch 413/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1399 - regression_loss: 0.0024 - class_loss: 0.6878\n",
      "Epoch 00412: reducing learning rate to 0.001458000100683421.\n",
      "Epoch 00412: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1399 - regression_loss: 0.0023 - class_loss: 0.6876 - val_loss: 0.1394 - val_regression_loss: 6.8143e-04 - val_class_loss: 0.6938\n",
      "Epoch 414/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1410 - regression_loss: 0.0037 - class_loss: 0.6864Epoch 00413: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1409 - regression_loss: 0.0037 - class_loss: 0.6863 - val_loss: 0.1396 - val_regression_loss: 8.1142e-04 - val_class_loss: 0.6938\n",
      "Epoch 415/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1390 - regression_loss: 0.0021 - class_loss: 0.6847Epoch 00414: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0021 - class_loss: 0.6843 - val_loss: 0.1395 - val_regression_loss: 6.6591e-04 - val_class_loss: 0.6943\n",
      "Epoch 416/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0031 - class_loss: 0.6822Epoch 00415: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0031 - class_loss: 0.6817 - val_loss: 0.1397 - val_regression_loss: 7.0523e-04 - val_class_loss: 0.6948\n",
      "Epoch 417/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1383 - regression_loss: 0.0019 - class_loss: 0.6821Epoch 00416: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1383 - regression_loss: 0.0019 - class_loss: 0.6823 - val_loss: 0.1398 - val_regression_loss: 7.6202e-04 - val_class_loss: 0.6952\n",
      "Epoch 418/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1413 - regression_loss: 0.0050 - class_loss: 0.6812Epoch 00417: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1415 - regression_loss: 0.0049 - class_loss: 0.6826 - val_loss: 0.1396 - val_regression_loss: 6.1363e-04 - val_class_loss: 0.6948\n",
      "Epoch 419/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1386 - regression_loss: 0.0020 - class_loss: 0.6826Epoch 00418: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0020 - class_loss: 0.6819 - val_loss: 0.1396 - val_regression_loss: 6.6394e-04 - val_class_loss: 0.6946\n",
      "Epoch 420/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0035 - class_loss: 0.6851Epoch 00419: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0034 - class_loss: 0.6844 - val_loss: 0.1395 - val_regression_loss: 6.1301e-04 - val_class_loss: 0.6945\n",
      "Epoch 421/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0034 - class_loss: 0.6817Epoch 00420: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0033 - class_loss: 0.6811 - val_loss: 0.1398 - val_regression_loss: 6.5783e-04 - val_class_loss: 0.6956\n",
      "Epoch 422/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1390 - regression_loss: 0.0023 - class_loss: 0.6837Epoch 00421: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0023 - class_loss: 0.6838 - val_loss: 0.1397 - val_regression_loss: 6.4676e-04 - val_class_loss: 0.6955\n",
      "Epoch 423/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1425 - regression_loss: 0.0059 - class_loss: 0.6830Epoch 00422: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1425 - regression_loss: 0.0058 - class_loss: 0.6837 - val_loss: 0.1396 - val_regression_loss: 7.2659e-04 - val_class_loss: 0.6944\n",
      "Epoch 424/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1419 - regression_loss: 0.0051 - class_loss: 0.6837Epoch 00423: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1417 - regression_loss: 0.0050 - class_loss: 0.6832 - val_loss: 0.1394 - val_regression_loss: 6.3572e-04 - val_class_loss: 0.6936\n",
      "Epoch 425/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0028 - class_loss: 0.6840Epoch 00424: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0028 - class_loss: 0.6837 - val_loss: 0.1396 - val_regression_loss: 8.4645e-04 - val_class_loss: 0.6936\n",
      "Epoch 426/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0023 - class_loss: 0.6810Epoch 00425: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1385 - regression_loss: 0.0023 - class_loss: 0.6808 - val_loss: 0.1395 - val_regression_loss: 7.0011e-04 - val_class_loss: 0.6940\n",
      "Epoch 427/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1422 - regression_loss: 0.0051 - class_loss: 0.6854Epoch 00426: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1418 - regression_loss: 0.0050 - class_loss: 0.6840 - val_loss: 0.1396 - val_regression_loss: 6.5311e-04 - val_class_loss: 0.6947\n",
      "Epoch 428/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1395 - regression_loss: 0.0025 - class_loss: 0.6850Epoch 00427: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0025 - class_loss: 0.6848 - val_loss: 0.1395 - val_regression_loss: 6.2122e-04 - val_class_loss: 0.6945\n",
      "Epoch 429/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0022 - class_loss: 0.6829Epoch 00428: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1388 - regression_loss: 0.0022 - class_loss: 0.6831 - val_loss: 0.1394 - val_regression_loss: 6.5106e-04 - val_class_loss: 0.6938\n",
      "Epoch 430/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0020 - class_loss: 0.6839Epoch 00429: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1389 - regression_loss: 0.0020 - class_loss: 0.6843 - val_loss: 0.1398 - val_regression_loss: 9.9434e-04 - val_class_loss: 0.6939\n",
      "Epoch 431/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0022 - class_loss: 0.6812Epoch 00430: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0022 - class_loss: 0.6812 - val_loss: 0.1395 - val_regression_loss: 6.8151e-04 - val_class_loss: 0.6939\n",
      "Epoch 432/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1392 - regression_loss: 0.0027 - class_loss: 0.6825Epoch 00431: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0026 - class_loss: 0.6819 - val_loss: 0.1394 - val_regression_loss: 6.4783e-04 - val_class_loss: 0.6938\n",
      "Epoch 433/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1384 - regression_loss: 0.0020 - class_loss: 0.6819Epoch 00432: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0020 - class_loss: 0.6821 - val_loss: 0.1394 - val_regression_loss: 6.8509e-04 - val_class_loss: 0.6935\n",
      "Epoch 434/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1397 - regression_loss: 0.0032 - class_loss: 0.6822Epoch 00433: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0032 - class_loss: 0.6812 - val_loss: 0.1394 - val_regression_loss: 6.2704e-04 - val_class_loss: 0.6940\n",
      "Epoch 435/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1393 - regression_loss: 0.0035 - class_loss: 0.6790Epoch 00434: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1393 - regression_loss: 0.0034 - class_loss: 0.6793 - val_loss: 0.1397 - val_regression_loss: 5.8951e-04 - val_class_loss: 0.6955\n",
      "Epoch 436/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1382 - regression_loss: 0.0023 - class_loss: 0.6794Epoch 00435: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1382 - regression_loss: 0.0023 - class_loss: 0.6794 - val_loss: 0.1400 - val_regression_loss: 6.9864e-04 - val_class_loss: 0.6963\n",
      "Epoch 437/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1402 - regression_loss: 0.0041 - class_loss: 0.6801Epoch 00436: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1401 - regression_loss: 0.0041 - class_loss: 0.6800 - val_loss: 0.1398 - val_regression_loss: 6.8919e-04 - val_class_loss: 0.6955\n",
      "Epoch 438/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1381 - regression_loss: 0.0024 - class_loss: 0.6785Epoch 00437: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1381 - regression_loss: 0.0024 - class_loss: 0.6786 - val_loss: 0.1398 - val_regression_loss: 8.0859e-04 - val_class_loss: 0.6949\n",
      "Epoch 439/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0020 - class_loss: 0.6823Epoch 00438: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1386 - regression_loss: 0.0020 - class_loss: 0.6830 - val_loss: 0.1396 - val_regression_loss: 8.2023e-04 - val_class_loss: 0.6939\n",
      "Epoch 440/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1396 - regression_loss: 0.0030 - class_loss: 0.6831Epoch 00439: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1395 - regression_loss: 0.0029 - class_loss: 0.6830 - val_loss: 0.1394 - val_regression_loss: 7.4585e-04 - val_class_loss: 0.6932\n",
      "Epoch 441/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1377 - regression_loss: 0.0018 - class_loss: 0.6796Epoch 00440: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1378 - regression_loss: 0.0018 - class_loss: 0.6798 - val_loss: 0.1394 - val_regression_loss: 6.9376e-04 - val_class_loss: 0.6937\n",
      "Epoch 442/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0025 - class_loss: 0.6816Epoch 00441: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0025 - class_loss: 0.6823 - val_loss: 0.1394 - val_regression_loss: 7.9597e-04 - val_class_loss: 0.6930\n",
      "Epoch 443/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1384 - regression_loss: 0.0024 - class_loss: 0.6798Epoch 00442: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0024 - class_loss: 0.6797 - val_loss: 0.1397 - val_regression_loss: 0.0010 - val_class_loss: 0.6932\n",
      "Epoch 444/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0023 - class_loss: 0.6826Epoch 00443: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1388 - regression_loss: 0.0023 - class_loss: 0.6828 - val_loss: 0.1394 - val_regression_loss: 7.7788e-04 - val_class_loss: 0.6931\n",
      "Epoch 445/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0022 - class_loss: 0.6814Epoch 00444: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1386 - regression_loss: 0.0022 - class_loss: 0.6817 - val_loss: 0.1393 - val_regression_loss: 7.0182e-04 - val_class_loss: 0.6928\n",
      "Epoch 446/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1406 - regression_loss: 0.0033 - class_loss: 0.6861Epoch 00445: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1405 - regression_loss: 0.0033 - class_loss: 0.6860 - val_loss: 0.1393 - val_regression_loss: 6.6056e-04 - val_class_loss: 0.6932\n",
      "Epoch 447/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0024 - class_loss: 0.6817Epoch 00446: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1387 - regression_loss: 0.0024 - class_loss: 0.6817 - val_loss: 0.1393 - val_regression_loss: 6.4252e-04 - val_class_loss: 0.6935\n",
      "Epoch 448/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1389 - regression_loss: 0.0023 - class_loss: 0.6833Epoch 00447: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1389 - regression_loss: 0.0023 - class_loss: 0.6832 - val_loss: 0.1393 - val_regression_loss: 6.9715e-04 - val_class_loss: 0.6932\n",
      "Epoch 449/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1392 - regression_loss: 0.0026 - class_loss: 0.6831Epoch 00448: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1392 - regression_loss: 0.0025 - class_loss: 0.6833 - val_loss: 0.1395 - val_regression_loss: 7.7302e-04 - val_class_loss: 0.6938\n",
      "Epoch 450/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0022 - class_loss: 0.6830Epoch 00449: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1387 - regression_loss: 0.0022 - class_loss: 0.6830 - val_loss: 0.1394 - val_regression_loss: 6.2514e-04 - val_class_loss: 0.6941\n",
      "Epoch 451/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1383 - regression_loss: 0.0020 - class_loss: 0.6814Epoch 00450: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1382 - regression_loss: 0.0020 - class_loss: 0.6812 - val_loss: 0.1396 - val_regression_loss: 6.6070e-04 - val_class_loss: 0.6945\n",
      "Epoch 452/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1387 - regression_loss: 0.0020 - class_loss: 0.6832Epoch 00451: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1388 - regression_loss: 0.0020 - class_loss: 0.6840 - val_loss: 0.1394 - val_regression_loss: 6.6658e-04 - val_class_loss: 0.6939\n",
      "Epoch 453/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0017 - class_loss: 0.6840Epoch 00452: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1385 - regression_loss: 0.0017 - class_loss: 0.6839 - val_loss: 0.1395 - val_regression_loss: 7.2446e-04 - val_class_loss: 0.6936\n",
      "Epoch 454/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1391 - regression_loss: 0.0021 - class_loss: 0.6846Epoch 00453: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0021 - class_loss: 0.6844 - val_loss: 0.1395 - val_regression_loss: 6.8076e-04 - val_class_loss: 0.6940\n",
      "Epoch 455/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1389 - regression_loss: 0.0023 - class_loss: 0.6828Epoch 00454: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1387 - regression_loss: 0.0023 - class_loss: 0.6822 - val_loss: 0.1396 - val_regression_loss: 8.4949e-04 - val_class_loss: 0.6937\n",
      "Epoch 456/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1382 - regression_loss: 0.0019 - class_loss: 0.6815Epoch 00455: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1382 - regression_loss: 0.0019 - class_loss: 0.6818 - val_loss: 0.1396 - val_regression_loss: 7.9478e-04 - val_class_loss: 0.6940\n",
      "Epoch 457/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1386 - regression_loss: 0.0027 - class_loss: 0.6795Epoch 00456: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1387 - regression_loss: 0.0027 - class_loss: 0.6800 - val_loss: 0.1393 - val_regression_loss: 6.7828e-04 - val_class_loss: 0.6932\n",
      "Epoch 458/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1423 - regression_loss: 0.0054 - class_loss: 0.6842Epoch 00457: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1422 - regression_loss: 0.0054 - class_loss: 0.6842 - val_loss: 0.1395 - val_regression_loss: 8.6731e-04 - val_class_loss: 0.6931\n",
      "Epoch 459/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1450 - regression_loss: 0.0096 - class_loss: 0.6768Epoch 00458: val_loss improved from 0.13920 to 0.13919, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1448 - regression_loss: 0.0094 - class_loss: 0.6769 - val_loss: 0.1392 - val_regression_loss: 5.3875e-04 - val_class_loss: 0.6933\n",
      "Epoch 460/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1391 - regression_loss: 0.0025 - class_loss: 0.6828Epoch 00459: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1391 - regression_loss: 0.0025 - class_loss: 0.6829 - val_loss: 0.1392 - val_regression_loss: 6.0922e-04 - val_class_loss: 0.6930\n",
      "Epoch 461/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1390 - regression_loss: 0.0032 - class_loss: 0.6788Epoch 00460: val_loss improved from 0.13919 to 0.13919, saving model to model.hdf5\n",
      "1580/1580 [==============================] - 2s - loss: 0.1387 - regression_loss: 0.0032 - class_loss: 0.6779 - val_loss: 0.1392 - val_regression_loss: 5.5478e-04 - val_class_loss: 0.6932\n",
      "Epoch 462/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1387 - regression_loss: 0.0020 - class_loss: 0.6833Epoch 00461: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1386 - regression_loss: 0.0020 - class_loss: 0.6833 - val_loss: 0.1392 - val_regression_loss: 5.8160e-04 - val_class_loss: 0.6933\n",
      "Epoch 463/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1393 - regression_loss: 0.0032 - class_loss: 0.6807\n",
      "Epoch 00462: reducing learning rate to 0.0013122001430019737.\n",
      "Epoch 00462: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1394 - regression_loss: 0.0031 - class_loss: 0.6812 - val_loss: 0.1393 - val_regression_loss: 6.9057e-04 - val_class_loss: 0.6930\n",
      "Epoch 464/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1379 - regression_loss: 0.0018 - class_loss: 0.6806Epoch 00463: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1382 - regression_loss: 0.0019 - class_loss: 0.6817 - val_loss: 0.1395 - val_regression_loss: 8.3073e-04 - val_class_loss: 0.6931\n",
      "Epoch 465/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1447 - regression_loss: 0.0086 - class_loss: 0.6802Epoch 00464: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1446 - regression_loss: 0.0084 - class_loss: 0.6808 - val_loss: 0.1393 - val_regression_loss: 7.6722e-04 - val_class_loss: 0.6928\n",
      "Epoch 466/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1405 - regression_loss: 0.0039 - class_loss: 0.6831Epoch 00465: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1403 - regression_loss: 0.0038 - class_loss: 0.6826 - val_loss: 0.1392 - val_regression_loss: 5.9035e-04 - val_class_loss: 0.6931\n",
      "Epoch 467/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1390 - regression_loss: 0.0033 - class_loss: 0.6785Epoch 00466: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0033 - class_loss: 0.6788 - val_loss: 0.1392 - val_regression_loss: 5.9284e-04 - val_class_loss: 0.6931\n",
      "Epoch 468/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1390 - regression_loss: 0.0021 - class_loss: 0.6849Epoch 00467: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1391 - regression_loss: 0.0021 - class_loss: 0.6850 - val_loss: 0.1393 - val_regression_loss: 6.1044e-04 - val_class_loss: 0.6932\n",
      "Epoch 469/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1372 - regression_loss: 0.0021 - class_loss: 0.6758Epoch 00468: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1373 - regression_loss: 0.0020 - class_loss: 0.6764 - val_loss: 0.1393 - val_regression_loss: 6.7568e-04 - val_class_loss: 0.6931\n",
      "Epoch 470/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1377 - regression_loss: 0.0020 - class_loss: 0.6785Epoch 00469: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1376 - regression_loss: 0.0020 - class_loss: 0.6784 - val_loss: 0.1394 - val_regression_loss: 6.7992e-04 - val_class_loss: 0.6934\n",
      "Epoch 471/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1375 - regression_loss: 0.0022 - class_loss: 0.6764Epoch 00470: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1375 - regression_loss: 0.0022 - class_loss: 0.6769 - val_loss: 0.1395 - val_regression_loss: 7.4601e-04 - val_class_loss: 0.6936\n",
      "Epoch 472/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1381 - regression_loss: 0.0019 - class_loss: 0.6808Epoch 00471: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1380 - regression_loss: 0.0019 - class_loss: 0.6808 - val_loss: 0.1394 - val_regression_loss: 6.9204e-04 - val_class_loss: 0.6935\n",
      "Epoch 473/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1391 - regression_loss: 0.0028 - class_loss: 0.6815Epoch 00472: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1396 - regression_loss: 0.0032 - class_loss: 0.6821 - val_loss: 0.1395 - val_regression_loss: 8.6043e-04 - val_class_loss: 0.6931\n",
      "Epoch 474/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1467 - regression_loss: 0.0106 - class_loss: 0.6803Epoch 00473: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1463 - regression_loss: 0.0104 - class_loss: 0.6796 - val_loss: 0.1394 - val_regression_loss: 6.3274e-04 - val_class_loss: 0.6937\n",
      "Epoch 475/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1379 - regression_loss: 0.0019 - class_loss: 0.6799Epoch 00474: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1380 - regression_loss: 0.0019 - class_loss: 0.6806 - val_loss: 0.1394 - val_regression_loss: 6.7418e-04 - val_class_loss: 0.6937\n",
      "Epoch 476/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1386 - regression_loss: 0.0022 - class_loss: 0.6820Epoch 00475: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1385 - regression_loss: 0.0022 - class_loss: 0.6817 - val_loss: 0.1395 - val_regression_loss: 7.0232e-04 - val_class_loss: 0.6940\n",
      "Epoch 477/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1380 - regression_loss: 0.0018 - class_loss: 0.6811Epoch 00476: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1379 - regression_loss: 0.0017 - class_loss: 0.6806 - val_loss: 0.1396 - val_regression_loss: 7.7706e-04 - val_class_loss: 0.6943\n",
      "Epoch 478/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1459 - regression_loss: 0.0093 - class_loss: 0.6828Epoch 00477: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1455 - regression_loss: 0.0091 - class_loss: 0.6820 - val_loss: 0.1397 - val_regression_loss: 7.2477e-04 - val_class_loss: 0.6949\n",
      "Epoch 479/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1384 - regression_loss: 0.0025 - class_loss: 0.6797Epoch 00478: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1383 - regression_loss: 0.0024 - class_loss: 0.6794 - val_loss: 0.1396 - val_regression_loss: 6.5628e-04 - val_class_loss: 0.6947\n",
      "Epoch 480/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0029 - class_loss: 0.6779Epoch 00479: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0029 - class_loss: 0.6778 - val_loss: 0.1395 - val_regression_loss: 6.2486e-04 - val_class_loss: 0.6942\n",
      "Epoch 481/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1393 - regression_loss: 0.0034 - class_loss: 0.6797Epoch 00480: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1392 - regression_loss: 0.0033 - class_loss: 0.6791 - val_loss: 0.1394 - val_regression_loss: 5.8742e-04 - val_class_loss: 0.6940\n",
      "Epoch 482/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1380 - regression_loss: 0.0023 - class_loss: 0.6784Epoch 00481: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1377 - regression_loss: 0.0023 - class_loss: 0.6770 - val_loss: 0.1393 - val_regression_loss: 5.6543e-04 - val_class_loss: 0.6935\n",
      "Epoch 483/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1379 - regression_loss: 0.0030 - class_loss: 0.6749Epoch 00482: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1383 - regression_loss: 0.0029 - class_loss: 0.6767 - val_loss: 0.1393 - val_regression_loss: 5.6232e-04 - val_class_loss: 0.6938\n",
      "Epoch 484/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1381 - regression_loss: 0.0019 - class_loss: 0.6809Epoch 00483: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1381 - regression_loss: 0.0019 - class_loss: 0.6810 - val_loss: 0.1393 - val_regression_loss: 5.5152e-04 - val_class_loss: 0.6936\n",
      "Epoch 485/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1382 - regression_loss: 0.0019 - class_loss: 0.6815Epoch 00484: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1382 - regression_loss: 0.0019 - class_loss: 0.6815 - val_loss: 0.1395 - val_regression_loss: 6.7220e-04 - val_class_loss: 0.6939\n",
      "Epoch 486/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1468 - regression_loss: 0.0109 - class_loss: 0.6794Epoch 00485: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1466 - regression_loss: 0.0107 - class_loss: 0.6796 - val_loss: 0.1394 - val_regression_loss: 5.4520e-04 - val_class_loss: 0.6941\n",
      "Epoch 487/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1379 - regression_loss: 0.0020 - class_loss: 0.6793Epoch 00486: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1379 - regression_loss: 0.0020 - class_loss: 0.6796 - val_loss: 0.1396 - val_regression_loss: 5.6730e-04 - val_class_loss: 0.6951\n",
      "Epoch 488/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1371 - regression_loss: 0.0021 - class_loss: 0.6752Epoch 00487: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1372 - regression_loss: 0.0021 - class_loss: 0.6756 - val_loss: 0.1396 - val_regression_loss: 6.1142e-04 - val_class_loss: 0.6951\n",
      "Epoch 489/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1384 - regression_loss: 0.0024 - class_loss: 0.6797Epoch 00488: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0025 - class_loss: 0.6799 - val_loss: 0.1396 - val_regression_loss: 7.9451e-04 - val_class_loss: 0.6942\n",
      "Epoch 490/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1579 - regression_loss: 0.0221 - class_loss: 0.6788Epoch 00489: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1572 - regression_loss: 0.0215 - class_loss: 0.6784 - val_loss: 0.1395 - val_regression_loss: 7.1618e-04 - val_class_loss: 0.6939\n",
      "Epoch 491/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1382 - regression_loss: 0.0022 - class_loss: 0.6798Epoch 00490: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1381 - regression_loss: 0.0022 - class_loss: 0.6794 - val_loss: 0.1395 - val_regression_loss: 7.2420e-04 - val_class_loss: 0.6940\n",
      "Epoch 492/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1373 - regression_loss: 0.0023 - class_loss: 0.6752Epoch 00491: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1375 - regression_loss: 0.0023 - class_loss: 0.6757 - val_loss: 0.1395 - val_regression_loss: 7.2571e-04 - val_class_loss: 0.6940\n",
      "Epoch 493/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1385 - regression_loss: 0.0029 - class_loss: 0.6778Epoch 00492: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1384 - regression_loss: 0.0029 - class_loss: 0.6777 - val_loss: 0.1392 - val_regression_loss: 5.7287e-04 - val_class_loss: 0.6933\n",
      "Epoch 494/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1388 - regression_loss: 0.0027 - class_loss: 0.6806Epoch 00493: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1385 - regression_loss: 0.0026 - class_loss: 0.6792 - val_loss: 0.1393 - val_regression_loss: 5.5558e-04 - val_class_loss: 0.6935\n",
      "Epoch 495/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1380 - regression_loss: 0.0021 - class_loss: 0.6794Epoch 00494: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1380 - regression_loss: 0.0021 - class_loss: 0.6794 - val_loss: 0.1393 - val_regression_loss: 5.4322e-04 - val_class_loss: 0.6940\n",
      "Epoch 496/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1377 - regression_loss: 0.0024 - class_loss: 0.6764Epoch 00495: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1377 - regression_loss: 0.0024 - class_loss: 0.6768 - val_loss: 0.1393 - val_regression_loss: 5.9116e-04 - val_class_loss: 0.6934\n",
      "Epoch 497/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1391 - regression_loss: 0.0024 - class_loss: 0.6837Epoch 00496: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1390 - regression_loss: 0.0024 - class_loss: 0.6831 - val_loss: 0.1396 - val_regression_loss: 8.4545e-04 - val_class_loss: 0.6937\n",
      "Epoch 498/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1457 - regression_loss: 0.0098 - class_loss: 0.6794Epoch 00497: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1454 - regression_loss: 0.0096 - class_loss: 0.6792 - val_loss: 0.1396 - val_regression_loss: 7.2189e-04 - val_class_loss: 0.6945\n",
      "Epoch 499/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1382 - regression_loss: 0.0028 - class_loss: 0.6769Epoch 00498: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1381 - regression_loss: 0.0028 - class_loss: 0.6767 - val_loss: 0.1399 - val_regression_loss: 6.7854e-04 - val_class_loss: 0.6961\n",
      "Epoch 500/500\n",
      "1536/1580 [============================>.] - ETA: 0s - loss: 0.1381 - regression_loss: 0.0021 - class_loss: 0.6800Epoch 00499: val_loss did not improve\n",
      "1580/1580 [==============================] - 2s - loss: 0.1382 - regression_loss: 0.0020 - class_loss: 0.6807 - val_loss: 0.1398 - val_regression_loss: 6.0064e-04 - val_class_loss: 0.6962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhYiIhPo/OWQ1stzjLhEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12541db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = final_model.fit([X_train, X_train_text], [Y_train, Y_train2],\n",
    "        nb_epoch = 500, \n",
    "        batch_size = 256, \n",
    "        verbose=1, \n",
    "        validation_data=([X_test, X_test_text], [Y_test, Y_test2]), \n",
    "        callbacks=[reduce_lr, checkpointer], shuffle=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1249539e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0193209678432\n",
      "0.0773805465499\n",
      "3.25618028108\n"
     ]
    }
   ],
   "source": [
    "final_model.load_weights(\"model.hdf5\")\n",
    "pred = final_model.predict([X_test, X_test_text])[0]\n",
    "\n",
    "predicted = pred\n",
    "original = Y_test\n",
    "\n",
    "plt.title('Actual and predicted')\n",
    "plt.legend(loc='best')\n",
    "plt.plot(original, color='black', label = 'Original data')\n",
    "plt.plot(pred, color='blue', label = 'Predicted data')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(np.mean(np.square(predicted - original)))\n",
    "print(np.mean(np.abs(predicted - original)))\n",
    "print(np.mean(np.abs((original - predicted) / original)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
